\documentclass[10pt]{article}


\input{latex-pckg.tex}
\input{latex-macro.tex}

% ===================

\title{Variance in Variants: Propagating Genome Sequence Uncertainty
into Phylogenetic Lineage Assignment}
\author{David Champredon*, Devan Becker*, Connor Chato, Gopi Gugan, Art
Poon}
\date{*contributed equally}


\linenumbers
%\modulolinenumbers[2]


\begin{document}
\maketitle

\normalsize
\vspace{1cm}
\tableofcontents

\begin{abstract}
Genetic sequencing is subject to many different types of errors, but most analyses treat the resultant sequences as if they are known without error.
Next generation sequencing methods rely on significantly larger numbers of reads than previous sequencing methods in exchange for a loss of accuracy in each individual read.
Still, the coverage of such machines is imperfect and leaves uncertainty in many of the base calls. 
On top of this machine-level uncertainty, there is uncertainty induced by human error, such as errors in data entry or incorrect parameter settings.
In this work, we demonstrate that the uncertainty in sequencing techniques will affect downstream analysis and propose a straightforward method to propagate the uncertainty.

Our method uses a probabilistic matrix representation of individual sequences which incorporates base quality scores as a measure of uncertainty that naturally lead to resampling and replication as a framework for uncertainty propagation.
With the matrix representation, resampling possible base calls according to quality scores provides a bootstrap- or prior distribution-like first step towards genetic analysis.
Analyses based on these re-sampled sequences will include an more complete evaluation of the error involved in such analyses.

We demonstrate our resampling method on SARS-CoV-2 data.
The resampling procedures adds a linear computational cost to the analyses, but the large impact on the variance in downstream estimates makes it clear that ignoring this uncertainty may lead to overly confident conclusions.
We show that SARS-CoV-2 lineage designations via Pangolin are much less certain than the bootstrap support reported by Pangolin would imply and the clock rate estimates for SARS-CoV-2 are much more variable than reported.
\end{abstract}


\section{Introduction}

Generating a genetic sequence from a biological sample is a complex process.
Nucleic acids must be extracted from the sample while avoiding contamination by foreign material.
If working with RNA, then we must use a reverse transcriptase reaction (which has a high base misincorporation rate) to convert the RNA into DNA. TODO: confirm parenthesis element with Art.

Polymerase chain reaction (PCR) amplification is often employed to enrich the sample for the target of interest.
For next-generation sequencing (NGS) protocols, we have to generate a sequencing library for instance by random shearing of nucleic acids into fragments that are ligated onto special ``adaptors''.
NGS procedures such as sequencing by synthesis suffer from greater error rate relative to conventional Sanger dye-terminator sequencing, although these rates have continued to improve with new technologies \citep{fullerChallengesSequencingSynthesis2009}. TODO: More recent paper
In addition, the short reads produced by NGS platforms need to be aligned --- either by alignment against a reference genome, \emph{de novo} assembly, or a combination of the two --- to reconstruct a consensus sequence using one or more bioinformatic programs.
Errors can be introduced in any one of these steps \citep{beerenwinkelUltradeepSequencingAnalysis2011, oraweAccountingUncertaintyDNA2015}.


In some cases, naturally occurring variation, \ie genetic polymorphisms, or variation induced by experimental error is directly quantified and encoded into the output.
For example, mixed peaks in sequence chromatograms produced from dye terminator sequencing by capillary electrophoresis are assigned standard IUPAC codes (\eg Y for C or T) when the base calling program cannot determine which base is dominant \citep{NomenclatureIncompletelySpecified1986}.
\citet{ewingBaseCallingAutomatedSequencer1998} and \citet{richterichEstimationErrorsRaw1998} both argued that estimates of the base call quality, typically quantified as Phred quality scores ($Q=-10 \log_{10} P$, where $P$ is the estimated error probability), can be an accurate estimate of the number of errors that the machines at the time would make.
The accuracy of Phred quality scores has been disputed and other methods for quantifying error probabilities have been proposed \citep{liAdjustQualityScores2004, depristoFrameworkVariationDiscovery2011, liSNPDetectionMassively2009}. TODO: rephrase "disputed".
Nevertheless, Phred scores remain the standard means of reporting the estimated error probabilities for current sequencing platforms.
Generally, these scores are used to censor the base calls (\ie label them ``N'' rather than A, T, C or G) if the estimated probability of error exceeds a predefined threshold.
It is also common practice to remove the sequence from further analysis if the total number of censored bases exceeds a maximum tolerance; \eg \citet{doroninaPhylogeneticPositionEmended2005, robaskyRoleReplicatesError2014,oraweAccountingUncertaintyDNA2015}.
Some authors/tools use more sophisticated models, such as \citet{wuEstimatingErrorModels2017} who use statistical models that incorporate read depth to determine a probability of a sequencing error, but still use the resultant reads to form a consensus sequence with no measure of uncertainty.
Furthermore, some studies have extended the concept of per-base error probabilities to calculate the joint likelihoods of partial or full sequences.
For example, \citet{depristoFrameworkVariationDiscovery2011} and \citet{gompertHierarchicalBayesianModel2011} incorporate adjusted Phred scores into a likelihood framework to generate more accurate estimates of genetic diversity within a population; this approach has subsequently been used to develop new estimators of genetic diversity \citep{fumagalliQuantifyingPopulationGenetic2013a}. 
\citet{kuoEAGLEExplicitAlternative2018} recently used a similar approach to develop a statistical test of whether a given genome sequence is consistent with a specified alternative sequence.
In general, the reported error probabilities from NGS technologies are primarily used for filtering low quality sequences and improving alignment algorithms (which both result in a consensus sequence that is assumed to be error-free) or for hypothesis tests concerning small collections (usually pairs) of sequences.


The uncertainty present in the sequences are seldom propagated to downstream analyses.
For example, methods for sequence alignment and homology searches generally employ heuristic algorithms that utilize similarity scores that do not explicitly incorporate the probabilities of sequencing errors.
Moreover, methods to reconstruct the evolutionary relationships among sequences as a phylogenetic tree tend to interpret ambiguous base calls as completely missing data, although some exceptions are found in the literature, \eg \citet{depristoFrameworkVariationDiscovery2011}.
This problem is exacerbated when each sequence represents the consensus of diverse copies of a genome, such as rapidly evolving virus populations where genuine polymorphisms are confounded with sequencing error. 
See \citet{schneiderConsensusSequenceZen2002} for more criticisms of the use of consensus sequences, along with visualizations \citep[][called \emph{sequence logos}, ]{schneiderSequenceLogosNew1990} to display the deviations from a consensus.

Though rare, some studies have proposed methods for propogation of uncertainty from one step to later steps of an analysis.
\citet{oraweAccountingUncertaintyDNA2015} suggest methods for propagation of sequence-level uncertainty into determining whether two subjects have the same alleles, as well as estimating confidence intervals for allele frequencies. 
Another exception can be found in \citet{kuhnerCorrectingSequencingError2014}, who incorporate an assumed or estimated error rate for the entire sequence into the calculation of a phylogenetic tree and found that incorporation of errors makes the inferred branch lengths much closer to the true (simulated) branch lengths.
Though they did not use nucleotide-level uncertainty, \citet{gompertHierarchicalBayesianModel2011} incorporate the coverage of NGS technologies as part of the uncertainty of estimates for the frequency of alleles in a population.
\citet{clementGNUMAPAlgorithmUnbiased2010} present an alignment algorithm (called GNUMAP) that takes nucleotide-level uncertainty into account. 
Their method incorporates Position Weight Matrices into a method of scoring multiple possible matches against a reference genome in order to choose the best alignment. 
These studies are the exceptions, rather than the rules, and their methods have not yet attained widespread use.




We present a simple general-purpose framework that can be incorporated into any analysis of genetic sequence data.
This framework involves converting the uncertainty scores into a matrix of probabilities, and repeatedly sampling from this matrix and using the resultant samples in downstream analysis.
Unlike likelihood-based approaches, we do not make assumptions about the underlying patterns or distributions in the data.
In so doing, we can gain more accurate estimation of the errors at the expense of computation time.
Our technique is amenable to quality score adjustments prior to applying our methods.
We demonstrate the impact of propagating sequence uncertainty by applying our methods to the problem of classifying SARS-CoV-2 genomes into predefined clusters known as ``lineages'' \citep{rambautDynamicNomenclatureProposal2020}, several of which correspond to variants carrying mutations that are known to confer an advantage to virus transmission or infectivity.
We also analyse a collection of SARS-CoV-2 sequences to demonstrate that the estimated rate of new mutations is much more variable than studies relying on deterministic sequences would conclude.



\section{Methods}

\subsection{Probabilistic representation of sequences}

Here, we describe two theoretical frameworks to model sequence uncertainty at the \emph{nucleotide level} or at the \emph{sequence level}.
In both frameworks, the sequence of nucleotides from a biological sample is not treated as a single unambiguous observation (known without error), but rather as a collection of possible sequences weighted by their probability.

\subsubsection{Nucleotide-level uncertainty}

To represent the uncertainty at each position along the genome we introduce the following matrix, which we will refer to as a probabilistic sequence and denote $\nps$:

\begin{equation}
\nps = \bordermatrix{   & 1 & 2 & \ldots & \ell \cr
                \sq{A} & \nps_{A, 1} & \nps_{A, 2} & \ldots & \nps_{A, \ell} \cr
                \sq{C} & \nps_{C, 1} & \nps_{C, 2} & \ldots & \nps_{C, \ell} \cr
                \sq{G} & \nps_{G, 1} & \nps_{G, 2} & \ldots & \nps_{G, \ell} \cr
                \sq{T} & \nps_{T, 1} & \nps_{T, 2} & \ldots & \nps_{T, \ell} \cr 
                \sq{-} & \nps_{-, 1} & \nps_{-, 2} & \ldots & \nps_{-, \ell} \cr 
}\label{eq:nps}
\end{equation}

Each column represents a position in a nucleotide sequence of length $\ell$.
Each row represents one of the four nucleotides \sq{A,C,G,T}, as well as an empty position ``\sq{-}'' that symbolizes a recorded deletion rather than missing data.
Hence, $\nps$ is a $5\times\ell$ matrix.



The elements of the probability sequence represent the probability that a nucleotide exists at a given position:

\begin{equation}
\nps_{\sq{n},j} = \pr{\text{nucleotide \sq{n} is at position }j}
\end{equation}
with the special case for a deletion:

\begin{equation}
\nps_{\sq{-},j} = \pr{\text{empty position }j}.
\end{equation}
Note that we have for all $1\leq j \leq \ell$:
\begin{equation}
\sum_{n} \nps_{n, j} = 1
\end{equation}
Also, the sequence length is stochastic if $0<\nps_{\sq{-},i}<1$ for at least one $i$.
The probability that the sequence has the maximum length $\ell$ is $\prod_{i=1}^\ell (1-\nps_{\sq{-},i})$.
The nucleotide (or deletion) drawn at each position is independent from all the others, so there are up to $5^\ell$ possible different sequences for a given probabilistic nucleotide sequence, but these sequences are \emph{not} equally probable.


A major limitation of this probabilistic representation of a sequence is that we lose all information on linkage disequilibrium.
This is especially problematic for recording insertions because insertions with $L \ge 2$ nucleotides are treated as $L$ independent single nucleotide insertions.
Instead, we assume that every nucleotide is an independent observation.
For example, a probability sequence populated from short read data from a diverse population would not store the information that two polymorphisms were always observed in the same reads, \ie in complete linkage disequilibrium.
We also lose information about autocorrelation in sequencing error, such as clusters of miscalled bases associated with later cycles of sequencing-by-synthesis platforms.
Sequence chromatograms and base quality scores are affected by the same loss of information.

We note that this representation is similar to the ``CATG'' file type as described in \citet{kozlovModelsOptimizationsTools2018}, which indicates the likelihoods of each nucleotide in an aligned mapping for multiple taxa. 
This file type is able to be used by RAxML-NG to estimate an overall error rate which is then used to estimate phylogenetic trees.
Our probability sequence is also similar in concept to Position Weight Matrices \citep[PWMs, ][]{stormoUsePerceptronAlgorithm1982} which are built according to the frequency of each base at each position of a multiple alignment. 
Our construction differs in that we are creating one matrix per sequence where the entries are weighted according to error probability within that sequence, rather than one matrix for a collection of sequences. 
However, methods that accept PWMs will be applicable to our probability sequences (and \emph{vice-versa}).


\subsubsection{Sequence-level uncertainty}

A significant problem of storing probabilities at the level of individual nucleotides is that generating a sequence from this matrix requires drawing $\ell$ independent outcomes.
For example, the reference SARS-CoV-2 genome is 29,903 nucleotides, and a substantial number of naturally-occurring sequence insertions have been described.
Thus it would not be surprising if $\ell$ exceeded 30,000 nucleotides (nt).
The majority of these technically possible $5^\ell$ sequences are not biologically plausible.
Therefore, we formulate an ordered subset $\sps = (\sps_i)_{i\in\{1\ldots m\} }$ of the first $m$ most likely sequences, which are ranked in descending order by the joint probability of nucleotide composition.
Note that the sequences in $\sps$, $\sps_i$, do not necessarily have the same length.
The observed genetic sequence, $s^*$, is a sample from a specified discrete probability distribution $a$:
\begin{equation}
\pr{s^* = \sps_i | i ... m} = a(i)
\end{equation}
This compact and approximate representation drastically reduces the number of operations to one sample, after some pre-processing to calcualte $a$.
The observed plurality sequence $s^*$ (the sequence consisting of the most likely base at each position) is guaranteed to be a member of $\sps$ if $\nps_{s(j), j} > 0.5\;\forall\;j$ where $s(j)$ is the $j$-th nucleotide of $s^*$; indeed, it is guaranteed to be the highest ranked member $i=0$.
We refer to any member of the set $\sps$ as a \emph{\slps}.
Note that because $a$ is a probability distribution, we must have $\sum_{i=1}^m a(i) = 1$.
In other words, this probability is conditional on the sequence being in $\sps$.


For example, suppose that we have the following \nlps:
$$
\nps = 
\bordermatrix{
& \scriptscriptstyle{1} & \scriptscriptstyle{2}& \scriptscriptstyle{3}& \scriptscriptstyle{4} & \scriptscriptstyle{5} & \scriptscriptstyle{6} \cr
\sq{A} & 0.9 & 0.05   & 0.99 & 0 & 0 & 0.6\cr
\sq{C} & 0   & 0.8 & 0 & 0 & 0.1 & 0.1\cr
\sq{G} & 0.1 & 0.15 & 0 & 0.3 & 0.9 & 0\cr
\sq{T} & 0 & 0 & 0.01 & 0.7 & 0 & 0.3\cr
\sq{-} & 0 & 0 & 0 & 0 & 0 & 0\cr
}
$$
such that there are $2\times 3 \times 2^3 \times 3 = 144$ possible sequences.
The most likely sequence has the highest joint nucleotide probability: \sq{ACATGA} with probability 0.2694 ($0.9\times 0.8\times 0.99 \times 0.7 \times 0.9 \times 0.6$).
If there is a positive probability of deletion for at least one position, then the sequence has a variable length.
Large genomes or sequencing targets will result in vanishingly small probabilities for all sequences, and thus calculations on the log scale may be necessary to reduce the chance of numerical underflow.

TODO: \dc{Shouldn't we also give a simple example for a sequence-level uncertainty $\sps$?}, also calculate the probability of the $m$th sequence and normalized probability.


\subsubsection{Deletions and insertions}

By construction, the \nlps must be defined with its longest possible length.
Deletions are naturally modelled with our representation but insertions have to be modelled using deletion probabilities. 
\begin{equation}
\label{eq:indel}
\nps = 
\bordermatrix{
&\scriptscriptstyle{1} & \scriptscriptstyle{2}& \scriptscriptstyle{3}& \scriptscriptstyle{4} & \scriptscriptstyle{5} & \scriptscriptstyle{6} \cr
\sq{A} & 0 & 0   & 1 & 0    & 1 & 0\cr
\sq{C} & 1 & 0    & 0 & 0    & 0 & 0\cr
\sq{G} & 0 & 0.99 & 0 & 0    & 0 & 0\cr
\sq{T} & 0 & 0    & 0 & 0.01 & 0 & 1\cr
\sq{-} & 0 & 0.01 & 0 & 0.99 & 0 & 0\cr
}
\end{equation}

The low deletion probability for position 2 is straightforward to interpret: about 1\% of the time, nucleotide \sq{G} at position 2 is deleted.
The high deletion probability for position 4 means there is a 1\% chance of a \sq{T} insertion at this position (\autoref{tab:spsexample}).

\begin{table}[H]
\begin{center}
\begin{tabular}{ll}
\hline
\textbf{sequence} & \textbf{probability} \\
\hline
$\sps_1$ = \sq{CGAAT}  & $a(1) = 0.9799$ \\
$\sps_2$ = \sq{CAAT}   & $a(2) = 0.01$  \\
$\sps_3$ = \sq{CGATAT} & $a(3) = 0.01$ \\
$\sps_4$ = \sq{CATAT}  & $a(4) = 0.0001$  \\
\hline
\end{tabular}
\end{center}
\caption{Sequence-level probabilistic sequence defined by Equation \eqref{eq:indel}}
\label{tab:spsexample}
\end{table}









\subsection{Constructing the probability sequence}


\subsubsection{SAM files}

In most next-generation sequencing applications, the estimated probability of sequencing error is quantified with the quality (or ``Phred'') score attributed to each base call produced by sequencing instrument.
The quality score $Q$ is directly related to this estimated error probability: $\epsilon = 10^{-Q/10}$ \citep{ewingBaseCallingAutomatedSequencer1998}, where $Q$ typically ranges between 1 and 60 (with 60 being the lowest probability of error), depending on the sequencing platform and version of base-calling software.
It is important to note that this quality score only measures the probability of error from the machine; $1 - \epsilon$ is an estmate of the probability of no sequencing errors and does not account for any other source of error.

More formally, the probability that the base call is correct is expressed as: 
\begin{equation}
\label{eq:basecall}
\pr{\text{nucleotide}=X \,\,|\,\, \text{observed nucleotide} = X} = 1 - \epsilon
\end{equation}
Unfortunately, quality scores have no information on the probabilities of the three other possible nucleotides if the base call is incorrect.
In the absence of information about the other bases, we assume that these other probabilities are uniformly distributed.


Raw short read data are typically recorded in a FASTQ format that stores both the sequences (base calls) and base-specific quality scores.
Since the reads often correspond to different positions of the target nucleic acid, \eg randomly sheared genomic DNA, it is necessary to align the reads to identify base calls on different reads that represent the same genome position.
This alignment step can be accomplished by mapping reads to a reference genome, by the \emph{de novo} assembly of reads, or a hybrid approach that incorporates both methods.
The aligned outputs are frequently recorded in the tabular Sequence Alignment/Map (SAM) format \citep{liSequenceAlignmentMap2009}.
Each row represents a short read, including the raw nucleotide sequence and quality strings; the optimal placement of the read with respect to the reference sequence (as an integer offset); and the compact idiosyncratic gapped alignment report (CIGAR) string, an application-specific serialization of the edit operations required to align the read to the reference.
The SAM format contains much more information (\url{https://samtools.github.io/hts-specs/SAMv1.pdf}), but for our purposes we only need the placement, sequence, quality, and CIGAR string.


We employed the following procedure to construct the nucleotide-level probabilistic sequence from the contents of a SAM file.
We initialize aligned sequence and quality strings with '-' in all positions before the first read and after the last read, and '!', which corresponds to a quality score of 0 ($Q=0$), to all other positions.
Next, we tokenize the CIGAR string into length-operation tuples, which determine how bases and quality scores from the raw strings are appended to the aligned versions.
Deleted bases (`D` operations) are not assigned Phred scores, so we assume them to have 0 error probability.

Insertions (`I` operations) are non-trivial to include in the probabilistic sequence.
Consider a short read with two bases inserted at position $j$ (say, an \sq{A} at position $j+1$ and a \sq{T} at position $j+2$) and a short read with one insertion at position $j$ (say, a \sq{C}).
It is entirely ambiguous whether the single insertion (\sq{C}) aligns with the first insertion (\sq{A}) or the second insertion (\sq{T}) of the first short read. 
This is problematic for building up the matrix from reads aligned to the reference sequence.
It is conceptually and computationally simpler to start from a populated matrix and sampling insertions.
For our purposes, we only consider the alignment of these sequences with a reference sequence and thus do not consider insertions.

Some NGS platforms (e.g., Illumina) use paired-end reads where the same nucleic acid template is read in both directions.
In these situations, we simply adjust all values by a factor of one half.
For bases where the paired-end reads overlap, this has the effect of averaging the base probability $1-\epsilon$.
For example, if $1-\epsilon$ is 90\% for $\sq{A}$ in one read and 95\% $\sq{A}$ in its mate, then 0.925 is added to the $\sq{A}$ row in $\nps'$ (with the remaining 0.075 uniformly distributed across the other nucleotides.
If the two reads were 60\% $\sq{A}$ and 55\% $\sq{C}$ at the same position, then we would increment the corresponding column vector ($\sq{A}, \sq{T}, \sq{C}, \sq{G}$) by $(0.6/2, 0.1/2, 0.1/2, 0.1/2)$ for the first read and $(0.15/2, 0.15/2, 0.55/2, 0.15/2)$ for the second, resulting in an addition of $(0.375, 0.125, 0.325, 0.125)$ for this pair.
Bases outside of the overlapping region contribute a maximum of 0.5 to $\nps'$, because the base call on the other read is missing data.
This approach has the advantage of making the parsing of SAM files trivially parallelizable since we do not need to know how reads are paired.
In addition, the coverage calculated from $\nps'$ is scaled to the number of templates rather than the number of reads.


\subsubsection{Consensus sequence FASTQ files}
\label{fastq_construction}

Full length or partial genome sequences are now frequently the product of next-generation sequencing, by taking the consensus of the aligned or assembled read data.
However, the original read data are often not published alongside the consensus sequence.
Some consensus sequences are released in a format where the bases are annotated with quality scores, \eg FASTQ.
There are several programs that provide methods to convert a SAM file into a consensus FASTQ file \citep{liAdjustQualityScores2004, keithSimulatedAnnealingAlgorithm2002, liMappingShortDNA2008a}.
These programs use slightly different methods for generating consensus quality scores, but filter quality scores for the majority base.
For example, suppose there are three reads with the following base calls at position $j$: \sq{A} with $Q=30$, \sq{A} with $Q=31$, and \sq{C} with $Q=15$.
Calculation of the consensus quality score will thereby exclude the $Q=15$ value and report a quality score calculated from $Q=30$ and $Q=31$, with the details of the calculation differing by software.


This omission makes it challenging for us to generate an $\nps$ matrix from a consensus FASTQ file.
Given the consensus base and its associated quality score at position $j$, we must assume that the other bases are all equally likely with probability $\epsilon_j/3$ (similar to \cite{kuoEAGLEExplicitAlternative2018} and Chapter 5 of \citet{kozlovModelsOptimizationsTools2018}).
For example, let's assume the output sequence after fragment sequencing and alignment is \sq{ACATG} and its associated quality scores are respectively $Q=(60,30,50,10,40)$.
The probabilistic sequence is:
\begin{equation}
\nps = 
\bordermatrix{
&\scriptscriptstyle{1} & \scriptscriptstyle{2}& \scriptscriptstyle{3}& \scriptscriptstyle{4} & \scriptscriptstyle{5} \cr
\sq{A} & 1-10^{-6} & 10^{-3}/3  & 1-10^{-5} & 10^{-1}/3 & 10^{-4}/3  \cr
\sq{C} & 10^{-6}/3 & 1-10^{-3}  & 10^{-5}/3 & 10^{-1}/3 & 10^{-4}/3  \cr
\sq{G} & 10^{-6}/3 & 10^{-3}/3  & 10^{-5}/3 & 10^{-1}/3 & 1-10^{-4} \cr
\sq{T} & 10^{-6}/3 & 10^{-3}/3  & 10^{-5}/3 & 1-10^{-1} & 10^{-4}/3 \cr
\sq{-} & 0 & 0 & 0 & 0 & 0 \cr
}
\end{equation}
Usually, the genetic sequence \sq{ACATG} would be considered as certain and quality scores discarded.
In contrast, the probability of the sequence \sq{ACATG} is only 0.899 within the probabilistic sequence framework.


Incorporating deletions in the absence of raw data is also challenging.
If one is willing to assume global deletion rate, then it is possible to extend the parameterization of $\nps$.
For example, if the probability of a single nucleotide deletion is $d$, then the probability of the called base is $(1-d_j)(1-\epsilon_j)$ and the other three nucleotides have probability $(1-d)\epsilon_j/3$.
Hence, if we assume the base call is \sq{A}, the column of the \nlps for that position is
\begin{equation}
\nps(,j) = 
\bordermatrix{
&\scriptscriptstyle{j}  \cr
\sq{A} & (1 - d)(1-\epsilon_j) \cr
\sq{C} & (1 - d)\epsilon_j/3 \cr
\sq{G} & (1 - d)\epsilon_j/3 \cr
\sq{T} & (1 - d)\epsilon_j/3 \cr
\sq{-} & d \cr
}
\label{eq:deletion}
\end{equation}

Since the FASTQ file only has a single sequence, we do have the same issues with alignment of differing lengths of insertions.
In fact, insertions are only insertions relative to the reference sequence; they can simply be treated as observed nucleotides with an associated quality score.
It would be possible to give insertions special treatment, however, by defining a global insertion rate.
This insertion rate can be expressed as a deletion rate relative to the observed sequence, and thus one minus the insertion rate can be treated as the deletion rate in the probabilistic sequence.
As with the deletion rate, this requires an assumption about a global rate which may be arbitrary.


\subsubsection{Consensus sequence FASTA files}


If we do not have access to any base quality information, \eg the consensus sequence is published as a FASTA file, then our ability to populate $\nps$ is severely limited.
Any uncertainty that we impose upon the data will be a principled assumption.
The error probability at the $j$ position of the consensus sequence can be simulated as a beta distribution, \ie \vspace{-4mm}

$$
\epsilon_j \sim\text{Beta}(\alpha, \beta)
$$

The called base at position $j$ has probability $1-\epsilon_j$, and the remaining bases are assigned $\epsilon_j/3$.
To incorporate deletions, another probability $d$ can be generated as the \emph{gap probability}.
With these defined, the nucleotide-level probabilistic sequence at the $j$th column (assuming the base call at position $j$ was $\sq{A}$) can be written as above.
This probabilistic sequence is completely fabricated, \ie not based on any empirical data.
However, the sensitivity of an analysis can be evaluated by choosing different values of $\alpha$, $\beta$, and $d$ (\eg based on previous studies) and propagating these uncertainties into downstream analyses.
The results from such an analysis would not indicate anything about the sequence itself but could be used to determine how robust the methods are to increased sequence uncertainty.


\subsection{Propagation of uncertainty via resampling}

The most general way to propagate uncertainty is through resampling.
Given $\nps$ and assuming that individual nucleotides are independent outcomes --- or precomputing a reduced set of $m$ sequences and calculating the distribution $a$ of their joint probabilities, \ie the sequence-level approach --- we can propagate uncertainty by running downstream analyses on each set of sampled sequences.

At a nucleotide level, we are sampling from a multinomial distribution.
If the $j$th column of $\nps$ is (0.5, 0.2, 0.2, 0.09, 0.01), then we could sample $\sq{A}$ with 50\% probability, $\sq{C}$ with 20\%, etc.
As with other sequence analyses, we can censor the positions that do not have enough coverage.
We arbitrarily chose to censor any position that had fewer than 10 reads, which can be determined by summing the column of the probabilistic sequence.

In a maximum likelihood framework, this procedure is similar to bootstrapping.
In fact, the ultimate effect of this is to decrease the bootstrap confidence to a level that is more in line with the measured uncertainty in the base calls.

In a Bayesian framework, the multinomial sampling could be incorporated as prior distribution on each nucleotide.
For large collections of large sequences in an Markov Chain Monte Carlo algorithm, this increases the dimensionality dramatically.

\subsection{Implementation}

A C program has been written to convert SAM files into our matrix representation. The program assumes that the reads are aligned to a reference, then uses that reference to initiate the matrix. Because of our methods for handling paired reads, the program is able to stream the file line-by-line in a parallel computing environment. 

The resampling algorithm defined above has been implemented in the R programming language. A shell script is used to repeatedly call the necessary R functions and apply the resampling algorithm to all outputs of the C program until the desired number of samples is obtained. All of the code for this project is available at \url{https://github.com/Poonlab/SUP}.

\section{Applications}

\subsection{SARS-CoV-2 lineage assignemnt}

In this section, we apply the re-sampling method to evaluate the impact of sequencing error on the lineage assignments of SARS-CoV-2.
Sequences are sampled from $\nps$ and then assigned a lineage based on the current state-of-the-art in phylogenetic analysis.

We use the lineage designation algorithm described in \citet{rambautDynamicNomenclatureProposal2020} and assign our sequences to lineages using the pangoLEARN tool (Pangolin version 2.3.2, pangoLEARN version 2021-02-21) that the authors have made available (\url{github.com/cov-lineages/Pangolin}).
This tool uses a decision tree model to determine which lineage a given sequence is most likely to belong to.
We demonstrate that even the best available tools are underestimating the variance and therefore producing overconfident conclusions.

\subsubsection{Data}

The data for this application were downloaded from NCBI's SRA web interface (\url{https://www.ncbi.nlm.nih.gov/sra/?term=txid2697049}) on July 17th, 2021.
Search results were filtered to only include records that had SAM files so that our alignments were consistent with the originating work.
To select which runs to download, an arbitrary selection of 5-10 records from each of 20 non-sequential results pages were chosen.
Once collecting the run accession numbers from the search results, an R script was run to download the relevant files and check that all information was complete.
23 out of 275 files were incomplete due to technical errors during the download process and a further 4 were rejected due to lack of CIGAR strings. TODO: Justify: The files didn't properly download, so I skipped them. We didn't need them since we had plenty of other files. Is this a bad justification?
The GISAID accession numbers for the sequences we used are provided in the appendix. 


\subsubsection{Re-sampling the probabilistic sequence}


Since pangoLEARN is a pre-trained model, assigning lineage designations to a large number of resampled genome sequences is not computationally burdensome.
Sampling 5,000 different sequences from a probabilistic sequence can be done in a reasonable amount of time, even on a mid-range consumer laptop.

For this analysis we use the most basic resampling strategy described above.
We sample base calls from the multinomial distribution then use pangoLEARN to determine the lineage assignment.

\begin{figure}
\includegraphics[width=\textwidth]{figs/sampled_bars.pdf}

\caption{\label{fig:covidcalls}Visualization of called lineages from Pangolin.
Red bars indicate the lineage of the most probable sequence and grey bars represent other sequences called from the same SAM file.
Any lineage with fewer than 100 observations in the simulated sequences was grouped into the ``Other'' category.
There were 95 sequences total, but we only the plotted ones where the second most common lineage designation had more than 250 observations. }
\end{figure}


Figure \ref{fig:covidcalls} shows that the consensus sequence is almost always assigned the same lineage as the majority of the resamples; the full results are in Table \ref{tab:pango} in the appendix.
The proportion of resamples with the same lineage as the consensus sequence is very rarely 100\% and can be as low as 32.86\% (accession number ERR4440425). TODO: How many were 100\%?
TODO: Explain "None", find out how many were None, and explain why it's not in the graph (if None was one of the possibilities, then all of the samples were None).
It is noteworthy that the only times where 100\% of resampled sequences agreed are when the lineage call was ``None'' or for the lineage labelled B.1.1.7.
This lineage is a significantly more infectious lineage, represents blank\% of our data, TODO: what percent of the data, and is of special concern to health authorities TODO: Citation Needed. 

TODO: \dc{I'd like to suggest an alternative to Figure 1. A histogram where the x-axis is the proportion of resamples that are assigned the same lineage as the consensus sequence. This way we would have a figure that represent \emph{all} the data (not truncated to those that have more than 250 observations); this may also be more to the point: taking sequencing uncertainty into account, some (many?) sequences cannot be so confidently assigned to a single consensus sequence.
We could even add another panel (so that would be a 2-panel figure) with a histogram again, but this time the x-axis shows the number of lineages associated to the sequences with probability, say, greater than 5\%.}

\begin{figure}
\includegraphics[width=\textwidth]{figs/prop_correct.pdf}
\caption{\label{fig:covidcalls2}(First alternative to Figure 1)Proportion of resampled sequences that are assigned to the same lineage as the consensus sequence. One proportion is calculated for each SAM file. TODO: Only show parts of the plot to reduce whitespace; include this as well as Figure \ref{fig:covidcalls}.}
\end{figure}


\begin{figure}
\includegraphics[width=\textwidth]{figs/histograms.pdf}
\caption{\label{fig:covidcalls3}(Second alternative to Figure 1)Histograms representing the results from each SAM file. From left to right, top to bottom: the proportion of sampled sequences that were assigned the same lineage as the consensus sequence; the proportion of resamples assigned to the second most common lineage; the same histogram, but for the third most likely lineage; the number of lineages that were only observed once (atoms); the total number of different lineages observed; and the ratio of the number of resamples agreeing with the most common lineage divided by the number of lineags agreeing with the second most common (e.g., 5000 means that there were 5000 times as many lineages in the most common category versus the second most common category).}
\end{figure}



\subsection{Clock rate estimation for SARS-CoV-2}

TODO: Explanation of root-to-tip regression, including the purpose, interpretation of the clock rate and intercept (aka date of MRCA), and references for more information.  

The clock rate (slope of a root-to-tip regression) for SARS-CoV-2 is commonly estimated as a fixed rate near 0.001 mutations per site per year \citep{ducheneTemporalSignalPhylodynamic2020, choudharySevereAcuteRespiratory2021, songGenomicEpidemiologySARSCoV22021, niePhylogeneticPhylodynamicAnalyses2020, geidelbergGenomicEpidemiologyDensely2021}.
Using the same resampling methods as above, we estimate a clock rate for trees estimated from each of 50 resamples and for the tree estimated based on the reported consensus sequences.

To obtain the data, we sampled genomes uniformly from each month of recorded data in GenBank, using filters to ensure that the genomes were complete and had an associated SAM file.
We further had to filter out SAM files that were incomplete or did not contain the CIGAR strings necessary for alignment, leaving us with 244 sequences.
The associated GISAID accession numbers are provided in the Appendix.

Our re-sampling method will, by definition, introduce other possible mutations beyond what the consensus sequence suggests.
Because of this, the apparent number of mutations between a re-sampled genome and the estimated root is a function of the coverage, with more positions read or more uncertainty in the sequence leading to artificially inflated terminal branch lengths.
Furthermore, we are sampling nucleotides at each postion independently of other positions as well as independently of ancestral sequences. 
This implies that the estimates of the time for the most recent common ancestor are not reliable.
However, assuming that the sequences have comparable levels of uncertainty, each branch increases by a similar amount and the clock rate should not be affected, only the intercept. 

The sequences that we acquired did not have comparable levels of uncertainty; the viruses sampled early in the pandemic had considerably higher uncertainty, most likely due to a lack of consistent laboratory guidelines for sequencing this new virus.
To account for this, we calculated the sum of $\nps'$ for each sequence and applied Statistical Process Control techniques to ensure that all of the sequences had a similar level of coverage.
In particular, we calculated the mean coverage of the sequences in our data set, $\bar c$, and the standard deviation of the coverage, $s$. 
We removed any sequences outside of $\bar c \pm 3 s$, recalculated $\bar c$ and $s$, and iterated the removal process until all sequence coverages were within the bounds, amounting to 20 removed sequences.


The clock rate was estimated using TreeTime \citep{sagulenkoTreeTimeMaximumlikelihoodPhylodynamic2018}. 
We recorded the clock rate and standard error from the time tree constructed using the consensus sequences and compared this to the clock rate and standard deviations of the estimated clock rates in the resampled sequences. The tree built from consensus sequences had a clock rate of $6.5\times 10^{-4}$ with a standard error of $8.01\times 10^{-5}$. The mean of the clock rates for all of the sets of resampled sequences was $8.6\times 10^{-4}$ with standard deviation of $5.3\times 10^{-4}$, which is approximately 1.6 times as large as the standard error for the consensus sequences.


The estimates of the clock rate are shown in Figure \ref{fig:RTT_slope}. The red line and shaded region are the clock rate for the tree built from consensus sequences along with $\pm 1.96$ standard errors. Rate estimates from \citet{ducheneTemporalSignalPhylodynamic2020} (n=122), \citet{choudharySevereAcuteRespiratory2021} (n=261), \citet{songGenomicEpidemiologySARSCoV22021} (n=29), \citet{niePhylogeneticPhylodynamicAnalyses2020} (n=112), and \citet{geidelbergGenomicEpidemiologyDensely2021} (n=77) are also labelled on the plot with purple error bars for 95\% Bayesian Credible Intervals (BCI) or 95\% Highest Posterior Density (HPD), indicating that the rates and errors from each root-to-tip regression are in line with other published results.
Figure \ref{fig:RTT_slope} demonstrates that the estimated evolutionary rates have an average close to the rate estimated from the consensus sequences, but each of the individual error bars (from the five studies identified above) miss the excess variation due to sequence uncertainty.


\begin{figure}
\includegraphics[width=\textwidth]{figs/RTT_Slope.pdf}
\caption{\label{fig:RTT_slope}Clock rates (slope) and 95\% Confidence Intervals for the collections of re-sampled sequences. The red line and red shaded region are the clock rate and 95\% CI for the consensus sequences. The purple points and error bars are the clock rates and error intervals (either Bayesian Credible Interval or Highest Posterior Probability) from published studies, as labelled. The re-sampled sequences are in line with the consensus sequences as well as the published sequences, but represent a much larger variation due to the uncertainty in the original genome sequences.}
\end{figure}


\section{Conclusions}

The short run files produced by next generation sequencing platforms include valuable information about the quality of base calls which can easily be propagated into analyses.
In this study, we demonstrated that these errors in base calling can lead to different conclusions when determining a lineage via Pangolin and that the variance in clock rate estimates is larger than previously shown due to these errors.
Both of these situations could lead to incorrect conclusions, such as missing a variant of interest or making overconfident conclusions about the date of the first case of COVID-19.
The potential for errors in base calls should always be taken into account when making decisions based on genetic sequencing data.

The primary contribution of this research is the construction of the probability sequence, which allows for a wide variety of future research directions.
The direction we described here is focused on re-sampling, which allows a more complete appraisal of the variance in the estimates (or provides a reasonable prior distribution in a Bayesian setting), while comparing results for the most likely sequences provide a measure of robustness to sequence uncertainty.
However, this is not the only application of the probabilistic sequence.

Instead of SAM files, FASTQ files are often the best available quantification of the potential sequence read errors. 
We have already shown that probability sequences can be trivially constructed by spreading the remaining error probability uniformly across the remaining bases.
Another use of FASTQ files would be to construct a probability sequence as a reference genome for a given category.
This would entail collecting all available FASTQ files for a given lineage designation and using them in the construction of a probability sequence as if they were short reads in a SAM file.
From here, lineage designation for a newly acquired sequence (and its probability sequence) could be performed via a hypothesis test for whether the probability sequences are sufficiently similar. 

Our proposed methods can result in a linear increase in computational expense.
Even the method based on ordering the sequences by likelihood inevitably requries re-running the analysis numerous times. 
However, we have demonstrated that the uncertainty in the sequences themseleves can lead to major changes to the interpretations of the results.
The so-called ``consensus sequence'' is simply the most likely sequence, and the reported uncertainty is not merely an academic curiousity.
Ideally individual analyses would be constructed to take nucleotide-level uncertainty into account.
For instance, phylogenies have been estimated based on uncertain sequence information in \citet{rossOncoNEMInferringTumor2016, jahnTreeInferenceSinglecell2016, zafarSiFitInferringTumor2017} but the uncertainty is not derived from base quality scores.
An extension of these methods to incorporate the base quality scores may be a worthwhile research direction.

Computational burden can also be reduced by ording the sequences according to the sequence level uncertainty discussed above.
By putting all possible sequences in order of uncertainty, we can analyse how the biggest changes in uncertainty affect our analysis.
It is possible to devise an algorithm that puts the sequences in order of their uncertainty without calculating the uncertainty for every sequence.
The consensus sequence is clearly the least uncertain sequence, and by the construction of the probability sequence, it is computationally easy to calculate sequence uncertainty relative to this sequence.
Because of this, it should be reasonable to calculate all sequence uncertainties for a single mutation, then all pairs of mutations, and so on (we note that it is possible for some pairs of mutations to be more likely than single mutations, thus this algorithm cannot readily calculate the $n$ most likely sequences).
This allows for easy sorting of the most likely sequences, assuming enough sequence uncertainties have been calculated.
With this sorted list, we can examine each sequence in increasing uncertainty in order to approximate the distribution of sequence uncertainties.
Any model that uses sequence data could be re-fit with each sequence to investigate the robustness of that model to sequence uncertainty.

Our analysis focused on designation of lineages according to the Pangolin model as well as estimation of the clock rate.
The importance of incorporating sequence uncertainty is not confined to these applications; any analysis involving sequenced genomes would benefit from some method of incoporating the uncertainty or including some measure of robustness.
For example, the estimated frequency of alleles in the population could be used as the probability sequence, then propagated into further analysis.

Our method does not preclude tertiary analyses to test for systematic errors or deviations from a Mendelian inheritance pattern assumption.
We cannot account for systematic errors, such as those present due to human errors \citep[\eg as noted in][]{IssuesSARSCoV2Sequencing2020}. 
Our method allows for adjustments of the base call quality score, such as in \citet{brockmanQualityScoresSNP2008}, as well as more sophisticated definitions of genome likelihoods \citep[\eg ][]{liAdjustQualityScores2004, depristoFrameworkVariationDiscovery2011, liSNPDetectionMassively2009}.

We have developed an algorithm to include insertion events in a re-sampling scheme, but many of the resultant sequences were not mappable to known sequences.
The Pangolin lineage assignment system appears to treat insertions differently from single nucleotide polymorphisms, and our method of sampling insertions is incompatible with their treatment of insertions in lineage assignment operations.
This is potentially because the sampled base pair at any given position is independent of each other position, and the insertions observed in real-world data are possibly always associated with particular mutations elsewhere.

This study should not be taken in any way as a criticism of the Pangolin lineage assignment procedure.
Rather, Pangolin was chosen as it is a state-of-the art tool based on the best available algorithms for phylogeny reconstruction.
The phylogeny created by this team has been a vital resource for researchers and for public health professionals.
In particular, their label for the current Variants of Concern (VOCs), especially B.1.1.7, are the labels being used worldwide by news organizations.
The output from Pangolin and many other bioinformatics tools are usually interpreted as \emph{deterministic} results.
This study is an argument that inherent uncertainty in sequencing warrants propagation into downstream analyses.



\section*{Appendix}

The following table lists all of the GISAID accession numbers used in this paper.


% latex table generated in R 4.0.4 by xtable 1.8-4 package
% Tue Nov 16 10:42:23 2021
\begin{table}[ht]
\centering
\small
\begin{tabular}{llllll}
ERR4085809 & ERR4890354 & ERR4892048 & ERR5064166 & ERR5082590 & SRR13021020 \\ 
  ERR4204823 & ERR4890371 & ERR4892066 & ERR5064294 & ERR5082598 & SRR13021022 \\ 
  ERR4305816 & ERR4890386 & ERR4892112 & ERR5064346 & ERR5082599 & SRR13021027 \\ 
  ERR4307842 & ERR4890403 & ERR4892152 & ERR5064787 & ERR5082600 & SRR13021032 \\ 
  ERR4440194 & ERR4890427 & ERR4892200 & ERR5064811 & ERR5082606 & SRR13021033 \\ 
  ERR4440219 & ERR4890531 & ERR4892203 & ERR5074314 & ERR5082610 & SRR13021035 \\ 
  ERR4440247 & ERR4890572 & ERR4892293 & ERR5076163 & ERR5082622 & SRR13021038 \\ 
  ERR4440332 & ERR4890609 & ERR4892339 & ERR5076748 & ERR5082630 & SRR13021042 \\ 
  ERR4440354 & ERR4890693 & ERR4892386 & ERR5077151 & ERR5082645 & SRR13021047 \\ 
  ERR4440373 & ERR4890746 & ERR4892392 & ERR5077411 & ERR5082654 & SRR13021052 \\ 
  ERR4440402 & ERR4890771 & ERR4892423 & ERR5077618 & ERR5082656 & SRR13021053 \\ 
  ERR4440425 & ERR4890819 & ERR4893013 & ERR5077713 & ERR5082673 & SRR13021059 \\ 
  ERR4440731 & ERR4890820 & ERR4893031 & ERR5077924 & ERR5082674 & SRR13021061 \\ 
  ERR4692420 & ERR4890881 & ERR4893033 & ERR5078210 & ERR5082694 & SRR13021067 \\ 
  ERR4692568 & ERR4890926 & ERR4893037 & ERR5078863 & ERR5082695 & SRR13021072 \\ 
  ERR4692877 & ERR4890974 & ERR4893080 & ERR5078897 & ERR5082696 & SRR13021073 \\ 
  ERR4692945 & ERR4891001 & ERR4893138 & ERR5079000 & ERR5082700 & SRR13021077 \\ 
  ERR4693014 & ERR4891011 & ERR4893184 & ERR5079423 & ERR5082702 & SRR13021084 \\ 
  ERR4693019 & ERR4891037 & ERR4893186 & ERR5079699 & ERR5082706 & SRR13021090 \\ 
  ERR4693495 & ERR4891061 & ERR4893197 & ERR5080131 & ERR5082708 & SRR13021093 \\ 
  ERR4693801 & ERR4891103 & ERR4893242 & ERR5080159 & ERR5082710 & SRR13021097 \\ 
  ERR4693865 & ERR4891178 & ERR4893353 & ERR5080327 & ERR5082711 & SRR13021098 \\ 
  ERR4694010 & ERR4891235 & ERR4893393 & ERR5080504 & ERR5082712 & SRR13021099 \\ 
  ERR4694330 & ERR4891238 & ERR4999251 & ERR5080893 & SRR11433882 & SRR13021104 \\ 
  ERR4694380 & ERR4891261 & ERR4999255 & ERR5080897 & SRR11433888 & SRR13021107 \\ 
  ERR4694400 & ERR4891304 & ERR4999275 & ERR5080913 & SRR11433893 & SRR13021109 \\ 
  ERR4694498 & ERR4891415 & ERR4999282 & ERR5080918 & SRR12639958 & SRR13021111 \\ 
  ERR4694556 & ERR4891433 & ERR5060778 & ERR5081077 & SRR12639961 & SRR13021113 \\ 
  ERR4694571 & ERR4891444 & ERR5062004 & ERR5081293 & SRR12749715 & SRR13021115 \\ 
  ERR4694617 & ERR4891493 & ERR5062062 & ERR5081301 & SRR12749716 & SRR13021124 \\ 
  ERR4759453 & ERR4891497 & ERR5062388 & ERR5081304 & SRR12762573 & SRR13021130 \\ 
  ERR4869446 & ERR4891532 & ERR5062514 & ERR5081316 & SRR13020989 & SRR13021131 \\ 
  ERR4869458 & ERR4891572 & ERR5062571 & ERR5081322 & SRR13020990 & SRR13021133 \\ 
  ERR4869480 & ERR4891675 & ERR5062648 & ERR5081836 & SRR13020991 & SRR13021134 \\ 
  ERR4869487 & ERR4891711 & ERR5062729 & ERR5082214 & SRR13020998 & SRR13021135 \\ 
  ERR4869497 & ERR4891715 & ERR5062935 & ERR5082346 & SRR13020999 & SRR13021143 \\ 
  ERR4890228 & ERR4891805 & ERR5063143 & ERR5082556 & SRR13021003 & SRR13092002 \\ 
  ERR4890271 & ERR4891841 & ERR5063165 & ERR5082561 & SRR13021008 & SRR13592146 \\ 
  ERR4890285 & ERR4891863 & ERR5063539 & ERR5082569 & SRR13021010 &  \\ 
  ERR4890294 & ERR4891889 & ERR5063807 & ERR5082576 & SRR13021011 &  \\ 
  ERR4890337 & ERR4891898 & ERR5063813 & ERR5082578 & SRR13021013 &  \\ 
  ERR4890352 & ERR4891988 & ERR5063922 & ERR5082580 & SRR13021017 &
\end{tabular}
\caption{\label{resampled_acc}Accession numbers for the resampling application.}
\end{table}

% latex table generated in R 4.0.4 by xtable 1.8-4 package
% Tue Nov 16 10:43:08 2021
\begin{table}[ht]
\centering
\small
\begin{tabular}{llllll}
ERR4333012 & ERR4598849 & ERR4645575 & ERR4763502 & ERR4893972 & ERR5189526 \\ 
  ERR4333012 & ERR4598849 & ERR4645575 & ERR4763502 & ERR4893972 & ERR5189526 \\ 
  ERR4422411 & ERR4599609 & ERR4647168 & ERR4763917 & ERR4905630 & ERR5196271 \\ 
  ERR4422411 & ERR4599609 & ERR4647168 & ERR4763917 & ERR4905630 & ERR5196271 \\ 
  ERR4423341 & ERR4632053 & ERR4651160 & ERR4788047 & ERR4906265 & ERR5240413 \\ 
  ERR4423341 & ERR4632053 & ERR4651160 & ERR4788047 & ERR4906265 & ERR5240413 \\ 
  ERR4423907 & ERR4633143 & ERR4652052 & ERR4792808 & ERR4989718 & ERR5277731 \\ 
  ERR4423907 & ERR4633143 & ERR4652052 & ERR4792808 & ERR4989718 & ERR5277731 \\ 
  ERR4424340 & ERR4633439 & ERR4652877 & ERR4793403 & ERR5020396 & ERR5293387 \\ 
  ERR4424340 & ERR4633439 & ERR4652877 & ERR4793403 & ERR5020396 & ERR5293387 \\ 
  ERR4424995 & ERR4633631 & ERR4659368 & ERR4824581 & ERR5021500 & ERR5304264 \\ 
  ERR4424995 & ERR4633631 & ERR4659368 & ERR4824581 & ERR5021500 & ERR5304264 \\ 
  ERR4437121 & ERR4637060 & ERR4667806 & ERR4824816 & ERR5025790 & ERR5307708 \\ 
  ERR4437121 & ERR4637060 & ERR4667806 & ERR4824816 & ERR5025790 & ERR5307708 \\ 
  ERR4437452 & ERR4639632 & ERR4668406 & ERR4824949 & ERR5027044 & ERR5314268 \\ 
  ERR4437452 & ERR4639632 & ERR4668406 & ERR4824949 & ERR5027044 & ERR5314268 \\ 
  ERR4438147 & ERR4639682 & ERR4668440 & ERR4825023 & ERR5040813 & ERR5316846 \\ 
  ERR4438147 & ERR4639682 & ERR4668440 & ERR4825023 & ERR5040813 & ERR5316846 \\ 
  ERR4438486 & ERR4639875 & ERR4668990 & ERR4826433 & ERR5041080 & ERR5334211 \\ 
  ERR4438486 & ERR4639875 & ERR4668990 & ERR4826433 & ERR5041080 & ERR5334211 \\ 
  ERR4459703 & ERR4640333 & ERR4669218 & ERR4827255 & ERR5052912 & ERR5339018 \\ 
  ERR4459703 & ERR4640333 & ERR4669218 & ERR4827255 & ERR5052912 & ERR5339018 \\ 
  ERR4459988 & ERR4640467 & ERR4686528 & ERR4835054 & ERR5052951 & ERR5339175 \\ 
  ERR4459988 & ERR4640467 & ERR4686528 & ERR4835054 & ERR5052951 & ERR5339175 \\ 
  ERR4460366 & ERR4640673 & ERR4686632 & ERR4848811 & ERR5054027 & ERR5349715 \\ 
  ERR4460366 & ERR4640673 & ERR4686632 & ERR4848811 & ERR5054027 & ERR5349715 \\ 
  ERR4463279 & ERR4641513 & ERR4686760 & ERR4849464 & ERR5057371 & ERR5353067 \\ 
  ERR4463279 & ERR4641513 & ERR4686760 & ERR4849464 & ERR5057371 & ERR5353067 \\ 
  ERR4581201 & ERR4641559 & ERR4688435 & ERR4860936 & ERR5058153 & ERR5379217 \\ 
  ERR4581201 & ERR4641559 & ERR4688435 & ERR4860936 & ERR5058153 & ERR5379217 \\ 
  ERR4584371 & ERR4642761 & ERR4688535 & ERR4861339 & ERR5177079 & SRR12349113 \\ 
  ERR4584371 & ERR4642761 & ERR4688535 & ERR4861339 & ERR5177079 & SRR12349113 \\ 
  ERR4584814 & ERR4643065 & ERR4699831 & ERR4874605 & ERR5181042 & SRR12349131 \\ 
  ERR4584814 & ERR4643065 & ERR4699831 & ERR4874605 & ERR5181042 & SRR12349131 \\ 
  ERR4597698 & ERR4643184 & ERR4706873 & ERR4874858 & ERR5187190 &  \\ 
  ERR4597698 & ERR4643184 & ERR4706873 & ERR4874858 & ERR5187190 &  \\ 
  ERR4597906 & ERR4644945 & ERR4763252 & ERR4878261 & ERR5188799 &  \\ 
  ERR4597906 & ERR4644945 & ERR4763252 & ERR4878261 & ERR5188799 &
\end{tabular}
\caption{\label{rtt_acc}Accession numbers for the root-to-tip application.}
\end{table}

\normalsize\clearpage

\bibstyle{apa}
\bibliography{supbib}



\end{document}  