---
title: "Variance in Variants: Propagating Genome Sequence Uncertainty into Phylogenetic Lineage Assignment"
author: "David Champredon&ast;, Devan Becker&ast;, Connor Chato, Gopi Gugan, Art Poon"
date: "&ast;contributed equally"
output:
    pdf_document:
        number_sections: yes
        keep_tex: yes
        includes:
            in_header: 
                - preamble.tex
#classoption:
documentclass: article
fontsize: 12pt
#    - table
#    - "aspectratio=1610"


bibliography: supbib.bib
#csl: apa.csl
abstract: |
    | Genetic sequencing is subject to many different types of errors, but most analyses treat the resultant sequences as if they are perfect. Since the process of sequencing is very difficult, modern machines rely on significantly larger numbers of reads rather than making each read significantly more accurate. Still, the coverage of such machines is imperfect and leaves uncertainty in many of the base calls. Furthermore, there are circumstances around the sequencing that can induce further problems. In this work, we demonstrate that the uncertainty in sequencing techniques will affect downstream analysis and propose a straightforward (if computationally expensive) method to propagate the uncertainty.
    | Our method uses a probabilistic matrix representation of individual sequences which incorporates base quality scores and makes various uncertainty propagation methods obvious and easy. With the matrix representation, resampling possible base calls according to quality scores provides a bootstrap- or prior distribution-like first step towards genetic analysis. Analyses based on these re-sampled sequences will include an honest evaluation of the error involved in such analyses.
    | We demonstrate our resampling method on HIV and SARS-CoV-2 data. The resampling procedures adds computational cost to the analyses, but the large impact on the variance in downstream estimates makes it clear that ignoring this uncertainty leads to invalid conclusions. For HIV data, we show that phylogenetic reconstructions are much more sensitive to sequence error uncertainty than previously believed, and for SARS-CoV-2 data we show that lineage designations via Pangolin are much less certain than the bootstrap support would imply.
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE)

library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(here)
library(patchwork)
```


```{r load_sampled, include=FALSE}

# Read in CSV files
lins <- read.csv(here("data", "output", "lins.csv"),
    header = TRUE)

taxons <- sort(unique(lins$taxon))

```

```{r load_ordered, include=FALSE}
ords <- lapply(
    list.files(here("data/pangordlineages"),
        full.names = TRUE),
    read.csv
    ) %>%
    bind_rows() %>%
    separate(col = taxon,
        into = c("dummy", "acc", "lik", "subs"),
        sep = "_") %>%
    mutate(nsubs = str_count(subs, "2"),
        lik = as.numeric(lik)) %>%
    group_by(acc) %>%
    mutate(order = 1:n()) %>%
    ungroup()

```

TODO: Switch to LaTeX (finally time to start worrying about formatting).
TODO: call probability matrix, uncertainty matrix, or (nucleotide-level) probability sequence - choose one and stick to it.


# Introduction

Generating a genetic sequence from a biological sample is a complex process that involves many steps.
Nucleic acids must be extracted from the sample while avoiding contamination by foreign material.
If working with RNA, then we must use a reverse transcriptase reaction to convert the RNA into DNA.
PCR amplification is often employed to enrich the sample for the target of interest.
For some next-generation sequencing (NGS) protocols, we have to generate a sequencing library by random shearing of nucleic acids into fragments that are ligated onto special 'adaptors'.
NGS procedures such as sequencing by synthesis suffer from greater error rate relative to conventional Sanger dye-terminator sequencing, although these rates have continued to improve with new technologies [@fullerChallengesSequencingSynthesis2009].
In addition, the short reads produced by NGS platforms need to be aligned &mdash; either by alignment against a reference genome, *de novo* assembly, or a combination of the two &mdash; to reconstruct a consensus sequence using one or more bioinformatic programs.
Errors can be introduced any one of these steps  (@beerenwinkelUltradeepSequencingAnalysis2011, @oraweAccountingUncertaintyDNA2015).


In some cases, naturally occurring variation, \ie genetic polymorphisms, or variation induced by experimental error is directly quantified and encoded into the output.
For example, mixed peaks in sequence chromatograms produced from dye terminator sequencing by capillary electrophoresis are assigned standard IUPAC codes (\eg Y for C or T, R for A or G) when the base calling program cannot determine which base is dominant (@NomenclatureIncompletelySpecified1986).
@ewingBaseCallingAutomatedSequencer1998 and @richterichEstimationErrorsRaw1998 both showed that estimates of the base call quality, typically quantified as Phred quality scores ($Q=-10 \log_{10} P$, where $P$ is the estimated error probability), can be an accurate estimate of the number of errors that the machines at the time would make.
Even though the accuracy of Phred quality scores has been disputed, and other methods for quantifying error probabilities have been proposed (@liAdjustQualityScores2004, @depristoFrameworkVariationDiscovery2011, @liSNPDetectionMassively2009), Phred scores remain the standard means of reporting the estimated error probabilities for current sequencing platforms.
Generally, these scores are used to censor the base calls (\ie label them "N" rather than A, T, C or G) if the estimated probability of error exceeds a predefined threshold.
It is also common practice to remove the sequence from further analysis if the total number of censored bases exceeds a maximum tolerance; \eg @doroninaPhylogeneticPositionEmended2005, @robaskyRoleReplicatesError2014, @oraweAccountingUncertaintyDNA2015.
Some authors/tools, such as @lippiPotentialPreanalyticalAnalytical2020, use a more sophisticated form of quality control, but still use the resultant reads to form a consensus sequence with no measure of uncertainty.
@clementGNUMAPAlgorithmUnbiased2010 present an alignment algorithm (GNUMAP) that takes nucleotide-level uncertainty into account by assigning a matching score to each match based on the probability scores from the output of Solexa and Illumina platforms (which are generally not publically available). 
Furthermore, some studies have extended the concept of per-base error probabilities to calculate the joint likelihood of partial or full sequences.
For example, @depristoFrameworkVariationDiscovery2011 and @gompertHierarchicalBayesianModel2011 incorporate adjusted Phred scores into a likelihood framework to generate more accurate estimates of population-level allele distributions; this approach has subsequently been used to develop new estimators for population genetic diversity (@fumagalliQuantifyingPopulationGenetic2013a).
@kuoEAGLEExplicitAlternative2018 recently used a similar approach to develop a statistical test of test whether a given genome sequence is consistent with an alternative sequence.
In general, the reported error probabilities from NGS technologies are primarily used for filtering low quality sequences and improving alignment algorithms (which both result in a consensus sequence that is treated as error-free) or for hypothesis tests concerning small collections (usually pairs) of sequences.


The uncertainty present in the sequences are seldom propagated to downstream analyses.
For example, methods for sequence alignment and homology searches generally employ heuristic algorithms that utilize similarity scores that do not explicitly incorporate the probabilities of sequencing errors.
Moreover, methods to reconstruct the evolutionary relationships among sequences as a phylogenetic tree tend to interpret ambiguous base calls as completely missing data, although some exceptions are found in the literature, \eg @depristoFrameworkVariationDiscovery2011.
This problem is exacerbated when each sequence represents the consensus of diverse copies of a genome, such as rapidly evolving virus populations where genuine polymorphisms are confounded with sequencing error. 
See @schneiderConsensusSequenceZen2002 for more criticisms of the use of consensus sequences, along with visualizations (called *sequence logos*, @schneiderSequenceLogosNew1990) to display the deviations from a consensus.

One notable exception is @oraweAccountingUncertaintyDNA2015, who suggest methods for propagation of sequence-level uncertainty into determining whether two subjects have the same alleles, as well as estimating confidence intervals for allele frequencies. 
Another exception can be found in @kuhnerCorrectingSequencingError2014, who incorporate an assumed or estimated error rate for the entire sequence into the calculation of a phylogenetic tree and found that incorporation of errors makes the inferred branch lengths much closer to the true (simulated) branch lengths.
Though they did not use nucleotide-level uncertainty, @gompertHierarchicalBayesianModel2011 incorporate the coverage of NGS technologies as part of the uncertainty of estimates for the frequency of allelles in a population.
@clementGNUMAPAlgorithmUnbiased2010 present an alignment algorithm (called GNUMAP) that takes nucleotide-level uncertainty into account. 
Their method incorporates Position Weight Matrices into a method of scoring matches against a reference genome. 
These studies are the exceptions, rather than the rules, and their methods have not yet obtained widespread use.




We present a simple general-purpose framework that can be incorporated into any analysis of genetic sequence data.
This framework involves converting the uncertainty scores into a matrix of probabilities, and repeatedly sampling from this matrix and using the resultant samples in downstream analysis.
Unlike likelihood-based approaches, we do not make assumptions about the underlying patterns in the data.
In so doing, we can gain more accurate estimation of the errors at the expense of computation time.
Our technique is amenable to any appropriate quality score adjustments prior to building the uncertainty matrix.
We demonstrate the impact of propagating sequence uncertainty by applying our methods to the problem of classifying SARS-CoV-2 genomes into predefined clusters known as 'lineages', several of which correspond to variants carrying mutations that are known to confer an advantage to virus transmission or infectivity.



# Methods

## Probabilistic Representation of Sequences

Here, we describe two theoretical frameworks to model sequence uncertainty at the \emph{nucleotide level} or at the \emph{sequence level}.
In both frameworks, the sequence of nucleotides from a biological sample is not treated as a single unambiguous observation (known without error), but rather as a collection of possible sequences weighted by their probability.

### Nucleotide-level uncertainty

To represent the uncertainty at each location along the genome, we introduce following matrix:

\begin{equation}
\nps = \bordermatrix{   & 1 & 2 & \ldots & \ell \cr
                \sq{A} & \nps_{A, 1} & \nps_{A, 2} & \ldots & \nps_{A, \ell} \cr
                \sq{C} & \nps_{C, 1} & \nps_{C, 2} & \ldots & \nps_{C, \ell} \cr
                \sq{G} & \nps_{G, 1} & \nps_{G, 2} & \ldots & \nps_{G, \ell} \cr
                \sq{T} & \nps_{T, 1} & \nps_{T, 2} & \ldots & \nps_{T, \ell} \cr 
                \sq{-} & \nps_{x, 1} & \nps_{x, 2} & \ldots & \nps_{x, \ell} \cr 
}\label{eq:nps}
\end{equation}

Each column represents a position in a nucleotide sequence of length $\ell$.
Each row represents one of the four nucleotides \sq{A,C,G,T}, as well as an empty position "\sq{-}"" that symbolizes a genuine deletion, \ie not caused by missing data.
Hence, $\nps$ is a $5\times\ell$ matrix.



The elements of the uncertainty matrix represent the probability that a nucleotide is at given position:

\begin{equation}
\nps_{\sq{n},j} = \pr{\text{nucleotide \sq{n} is at position }j}
\end{equation}
with the special case for a deletion:

\begin{equation}
\nps_{\sq{-},j} = \pr{\text{empty position }j}.
\end{equation}
Note that we have for all $1\leq j \leq \ell$:
\begin{equation}
\sum_{n\in \{ \sq{A,C,G,T,-} \} } \nps_{n, j} = 1
\end{equation}
Also, the sequence length is stochastic if $0<\nps_{\sq{-},i}<1$ for at least one $i$.
The probability that the sequence has the maximum length $\ell$ is $\prod_{i=1}^\ell (1-\nps_{\sq{-},i})$.
We call the matrix $\nps$ the \emph{\nlps} of a biological sample.
The nucleotide (or deletion) drawn at each position is independent from all the others, so there are $5^\ell$ possible different sequences for a given probabilistic nucleotide sequence, but these sequences are *not* equally probable.


A major limitation of this probabilistic representation of a sequence is that we lose any information on linkage disequilibrium.
We assume that every nucleotide is an independent observation.
For example, a matrix populated from short read data from a diverse population would not store the information that two polymorphisms were always observed in the same reads, \ie in complete linkage disequilibrium.
We also lose information about autocorrelation in sequencing error, such as clusters of miscalled bases associated with later cycles of sequencing-by-synthesis platforms.
Sequence chromatograms are affected by the same loss of information.

We note that this representation is similar to the "CATG" file type as described in @kozlovModelsOptimizationsTools2018, which indicates the likelihoods of each nucleotide in an aligned mapping for multiple taxa. 
This file type is able to be used by RAxML-NG to estimate an overall error rate which is then used to estimate phylogenetic trees.
This matix is also similar in concept to Position Weight Matrices (PWMs, @stormoUsePerceptronAlgorithm1982) which are built according to the frequency of each base at each position of a multiple alignment. 
Our construction differs in that we are creating one matrix per sequence where the entries are weighted according to error probability within that sequence, rather than one matrix for a collection of sequences. 
However, methods that accept PWMs will be applicable to our uncertainty matrices (and *vice-versa*).


### Sequence-level uncertainty

A significant problem of storing probabilities at the level of individual nucleotides is that generating a sequence from this matrix requires drawing $\ell$ independent outcomes.
For example, the reference SARS-CoV-2 genome is 29,903 nucleotides, and a substantial number of naturally-occurring sequence insertions have been described.
Thus, we expect that $\ell$ exceeds 30,000.
The majority of these possible $5^\ell$ sequences are not biologically plausible.
Therefore, we formulate an ordered subset $\sps = (\sps_i)_{i\in\{1\ldots m\} }$ of the first $m$ most likely sequences, which are ranked in descending order by the joint probability of nucleotide composition.
Note that the $\sps_i$ do not have necessarily the same length.
The observed genetic sequence, $s$, is a sample from a specified discrete probability distribution $a$:
\begin{equation}
\pr{s = \sps_i} = a(i).
\end{equation}
This compact and approximate representation drastically reduces the number of operations to one sample, after some pre-processing to calcualte $a$.
The observed consensus sequence $s$ is guaranteed to be a member of $\sps$ if $\nps_{s(j), j} > 0.5\;\forall\;j$ where $s(j)$ is the $j$-th nucleotide of $s$; indeed, it is guaranteed to the highest ranked member $i=0$.
We refer to any member of the set $\sps$ as a \emph{\slps}.
Note that because $a$ is a probability distribution, we must have $\sum_{i=1}^m a(i) = 1$.
In other words, this probability is conditional on the sequence being in $\sps$.


For example, suppose that we have the following \nlps:
$$
\nps = 
\bordermatrix{
& \scriptscriptstyle{1} & \scriptscriptstyle{2}& \scriptscriptstyle{3}& \scriptscriptstyle{4} & \scriptscriptstyle{5} & \scriptscriptstyle{6} \cr
\sq{A} & 0.9 & 0.05   & 0.99 & 0 & 0 & 0.6\cr
\sq{C} & 0   & 0.8 & 0 & 0 & 0.1 & 0.1\cr
\sq{G} & 0.1 & 0.15 & 0 & 0.3 & 0.9 & 0\cr
\sq{T} & 0 & 0 & 0.01 & 0.7 & 0 & 0.3\cr
\sq{-} & 0 & 0 & 0 & 0 & 0 & 0\cr
}
$$
such that there are $2\times 3 \times 2^3 \times 3 = 144$ possible sequences.
The most likely sequence has the highest joint nucleotide probability: \sq{ACATGA} with probability 0.2694 ($0.9\times 0.8\times 0.99 \times 0.7 \times 0.9 \times 0.6$).
If there is a positive probability of deletion for at least one position, then the sequence has a variable length.
Let us take the same example as above, but modify one position to have a possible empty state:

$$
\nps = 
\bordermatrix{
&\scriptscriptstyle{1} & \scriptscriptstyle{2}& \scriptscriptstyle{3}& \scriptscriptstyle{4} & \scriptscriptstyle{5} & \scriptscriptstyle{6} \cr
\sq{A} & 0.9 & 0.05   & 0.99 & 0 & 0 & 0.6\cr
\sq{C} & 0   & 0.8 & 0 & 0 & 0.1 & 0.1\cr
\sq{G} & 0.1 & 0.15 & 0 & 0.2 & 0.9 & 0\cr
\sq{T} & 0 & 0 & 0.01 & 0.7 & 0 & 0.3\cr
\sq{-} & 0 & 0 & 0 & 0.1 & 0 & 0\cr
}.
$$

\noindent As above, there is still a 0.2694 probability that the sequence is \sq{ACATGA}, but now there is a chance that position 4 is deleted.
For example, the sequence is \sq{ACA-GA} with probability 0.038.

Below is an example for a \slps $\sps$ for $m=5$:
\begin{table}[H]
\begin{center}
\begin{tabular}{lc}
\hline
\textbf{sequence} & $a$ \\
\hline
\sq{ACATGA} & 0.60 \\
\sq{ACATCA} & 0.12 \\
\sq{AGATCA} & 0.15 \\
\sq{ACAGA}  & 0.05 \\
\sq{GCATGA} & 0.08 \\
\hline
\end{tabular}
\end{center}
\label{default}
\end{table}

For example, sampling from $\sps$ we expect to observe \sq{ACATCA} $12\%$ of the time.
Large genomes or sequencing targets will result in vanishingly small probabilities for all sequences, and thus calculations on the log scale may be necessary to reduce the chance of numerical underflow.


### Deletions and Insertions

By construction, the \nlps must be defined with its longest possible length.
Deletions are naturally modelled with our representation but insertions have to be modelled using deletion probabilities. 
\begin{equation}
\label{eq:indel}
\nps = 
\bordermatrix{
&\scriptscriptstyle{1} & \scriptscriptstyle{2}& \scriptscriptstyle{3}& \scriptscriptstyle{4} & \scriptscriptstyle{5} & \scriptscriptstyle{6} \cr
\sq{A} & 0 & 0   & 1 & 0    & 1 & 0\cr
\sq{C} & 1 & 0    & 0 & 0    & 0 & 0\cr
\sq{G} & 0 & 0.99 & 0 & 0    & 0 & 0\cr
\sq{T} & 0 & 0    & 0 & 0.01 & 0 & 1\cr
\sq{-} & 0 & 0.01 & 0 & 0.99 & 0 & 0\cr
}
\end{equation}

The low deletion probability for position 2 is straightforward to interpret: about 1\% of the time, nucleotide \sq{G} at position 2 is deleted.
The high deletion probability for position 4 means there is a 1\% chance of a \sq{T} insertion at this position (\autoref{tab:indelexample}).

\begin{table}[H]
\begin{center}
\label{tab:indelexample}
\begin{tabular}{ll}
\hline
\textbf{sequence} & \textbf{frequency} \\
\hline
\sq{CGAAT}  & common, 98\% of the time \\
\sq{CAAT}   & rare (1\% frequency) \sq{G} deletion at position 2,  \\
\sq{CGATAT} & rare (1\% frequency) \sq{T} insertion at position 4 \\
\sq{CATAT} & very rare (0.01\% frequency) deletion and insertion  \\
\hline
\end{tabular}
\end{center}
\caption{Representation of insertions and deletions from $\nps$ defined in Equation \eqref{eq:indel}}
\end{table}

The representation of deletions and insertions with a \slps (not nucleotide-level) is straightforward, because in this framework the sequences are explicitly written out &mdash; so are their deletions/insertions.



## Constructing The Uncertainty Matrix

<!--
### Sequence chromatograms
For sequences derived from conventional capillary-based methods, the uncertainty of base calls can be quantified from the sequence chromatogram.
-->


### SAM files
In most next-generation sequencing applications, the estimated probability of sequencing error is quantified with quality (or "Phred") score attributed to each base call produced by sequencing instrument.
The quality score $Q$ is directly related to the error probability: $\epsilon = 10^{-Q/10}$ (@ewingBaseCallingAutomatedSequencer1998), where $Q$ typically ranges between 1 and 60, depending on the sequencing platform and version of base-calling software.
More formally, the probability that the base call is correct is expressed as: 
\begin{equation}
\label{eq:basecall}
\pr{\text{nucleotide}=X \,\,|\,\, \text{observed nucleotide} = X} = 1 - \epsilon
\end{equation}
Unfortunately, quality scores have no information on the probabilities of the three other possible nucleotides if the base call is incorrect.
Therefore, we must assume that these probabilities are uniformly distributed.


Raw short read data are typically recorded in a FASTQ format that stores both the sequences (base calls) and base-specific quality scores.
Since the reads often correspond to different locations of the target nucleic acid, \eg randomly sheared genomic DNA, it is necessary to align the reads to identify base calls on different reads that represent the same genome position.
This alignment step can be accomplished by mapping reads to a reference genome, by the *de novo* assembly of reads, or a hybrid approach that incorporates both methods.
The aligned outputs are frequently recorded in the tabular Sequence Alignment/Map (SAM) format (@liSequenceAlignmentMap2009).
Each row represents a short read, including the raw nucleotide sequence and quality strings; the optimal placement of the read with respect to the reference sequence (as an integer offset); and the compact idiosyncratic gapped alignment report (CIGAR), an application-specific serialization of the edit operations required to align the read to the reference.


We employed the following procedure to construct the nucleotide-level probabilistic sequence from the contents of a SAM file.
We initialize aligned sequence and quality strings with '&ndash;' and '!' ($Q=0$) repeated $r-1$ times, respectively, where $r$ is the (one-index) position of the leftmost nucleotide in the reference genome.
Next, we tokenize the CIGAR string into length-operation tuples, which determine how bases and quality scores from the raw strings are appended to the aligned versions.
Deleted bases (`D` operations) are arbitrarily assigned a quality score of zero.
Insertions (`I` operations) are non-trivial to include in the sequence uncertatinty matrix.

Some NGS platforms use paired-end reads where the same nucleic acid template is read in both directions.
In these situations, we simply adjust all values by a factor of two.
For bases where the paired-end reads overlap, this has the effect of averaging the base probability $1-\epsilon$.
For example, if these values are 90% for $\sq{A}$ in one read and 95% $\sq{A}$ in its mate, then 0.925 is added to the $\sq{A}$ row in $\nps'$.
If the two reads were 60% $\sq{A}$ and 55% $\sq{C}$ at the same position, then we would increment the corresponding column vector by ($0.375$, $0.341\bar{6}$, $0.141\bar{6}$, $0.141\bar{6}$).
Bases outside of the overlapping region contribute a maximum of 0.5 to $\nps'$, because the base call on the other read is missing data.
This approach has the advantage of making the parsing of SAM files trivially parallelizable, since we do not need to know how reads are paired.
In addition, the coverage calculated from $\nps'$ is scaled to the number of templates, not the number of reads.


### Consensus sequence FASTQ files
\label{fastq_construction}

Full length or partial genome sequences are now frequently the product of next-generation sequencing, by taking the consensus of the aligned or assembled read data.
However, the original read data are often not published alongside the consensus sequence.
Some consensus sequences are released in a format where the bases are annotated with quality scores, \eg FASTQ.
There are several programs that provide methods to convert a SAM file into a consensus FASTQ file (@liAdjustQualityScores2004, @keithSimulatedAnnealingAlgorithm2002, @liMappingShortDNA2008a).
These programs use slightly different methods for generated consensus quality scores, but generally filter quality scores for the majority base.
For example, suppose there are three reads with the following base calls at position $j$: \sq{A} with $Q=30$, \sq{A} with $Q=31$, and \sq{C} with $Q=15$.
Calculation of the consensus quality score will thereby exclude the $Q=15$ value.


This omission makes it challenging for us to generate an $\nps$ matrix from a consensus FASTQ file.
Given the consensus base and its associated quality score at position $j$, we must assume that the other bases are all equally likely with probability $\epsilon_j/3$ (similar to @kuoEAGLEExplicitAlternative2018 and Chapter 5 of @kozlovModelsOptimizationsTools2018).
For example, let's assume the output sequence after fragment sequencing and alignment is \sq{ACATG} and its associated quality scores are respectively $Q=60,30,50,10,40$.
The probabilistic sequence is:
\begin{equation}
S = 
\begin{pmatrix}
1-10^{-6} & 10^{-3}/3  & 1-10^{-5} & 10^{-1}/3 & 10^{-4}/3  \\
10^{-6}/3 & 1-10^{-3}  & 10^{-5}/3 & 10^{-1}/3 & 10^{-4}/3  \\
10^{-6}/3 & 10^{-3}/3  & 10^{-5}/3 & 10^{-1}/3 & 1-10^{-4} \\
10^{-6}/3 & 10^{-3}/3  & 10^{-5}/3 & 1-10^{-1} & 10^{-4}/3 \\
0 & 0 & 0 & 0 & 0 \\
\end{pmatrix}
\end{equation}
Usually, the genetic sequence \sq{ACATG} would be considered as certain and quality scores discarded.
In contrast, the probability of the sequence \sq{ACATG} is only 0.899 within the probabilistic sequence framework.


Incorporating insertions and deletions ('indels') in the absence of raw data is also challenging.
If one is willing to assume global deletion rate, then it is possible to extend the parameterization of $\nps$.
For example, if the probability of a single nucleotide deletion is $d$, then the probability of the called base is $(1-d_j)(1-\epsilon_j)$ and the other three nucleotides have probability $(1-d)\epsilon_j/3$.
Hence, if we assume the base call is \sq{A}, the column of the \nlps for that position is
\begin{equation}
\nps(,j) = 
\bordermatrix{
&\scriptscriptstyle{j}  \cr
\sq{A} & (1 - d)(1-\epsilon_j) \cr
\sq{C} & (1 - d)\epsilon_j/3 \cr
\sq{G} & (1 - d)\epsilon_j/3 \cr
\sq{T} & (1 - d)\epsilon_j/3 \cr
\sq{-} & d \cr
}
\label{eq:deletion}
\end{equation}
Insertion rates are more challenging because the composition of the insertion sequence will vary by position, and even within a given position.


### Consensus sequence FASTA files


If we do not have access to any base quality information, \eg the consensus sequence is published as a FASTA file, then our ability to populate $\nps$ is severely limited.
Any uncertainty that we impose upon the data will be a principled assumption.
The error probability at the $j$ position of the consensus sequence can be simulated as a beta distribution, \ie 
$$
\epsilon_j \sim\text{Beta}(\alpha, \beta)
$$
The called base at position $j$ has probability $1-\epsilon$, and the remaining bases are assigned $\epsilon_j/3$.
To incorporate deletions, another probability $d$ can be generated as the *gap probability*.
With these defined, the nucleotide-level probabilistic sequence at the $j$th column (assuming the base call at location $j$ was $\sq{A}$) can be written as above.
This probabilistic sequence is completely fabricated, \ie not based on any empirical data.
However, the sensitivity of an analysis can be evaluated by choosing different values of $\alpha$, $\beta$, and $d$ (\eg based on previous studies) and propagating these uncertainties into downstream analyses.
The results from such an analysis would not indicate anything about the sequence itself but can determine how robust the methods are to increased sequence uncertainty.

<!--However, imposing our assumptions can still be very useful as a `stress test' of our analysis; we can assume different levels/types of uncertainty and quantify the stability of our results.-->


## Propagation of uncertainty via resampling

The most general way to propagate uncertainty is through resampling.
Given $\nps$ and assuming that individual nucleotides are independent outcomes &mdash; or precomputing a reduced set of $m$ sequences and calculating the distribution $a$ of their joint probabilities &mdash; we can propagate uncertainy by running downstream analyses on each set of sampled sequences.

At a nucleotide level, we are sampling from a multinomial distribution.
If the $j$th column of $\nps$ is (0.5, 0.2, 0.2, 0.09, 0.01), then we could sample $\sq{A}$ with 50\% probability, $\sq{C}$ with 20\%, etc.
However, this disregards the coverage at each location, thereby disregarding the variance of the error probabilities.
To incorporate the variance of the errors, a Dirichlet-Multinomial sampling scheme can be derived by assuming that the error probabilities follow a Dirichlet prior and the bases follow a multinomial likelihood. 
For each base at each location, the entry for $\nps'$ is defined as the sum of the probabilities (one minus the error probabilities). 
These values can be used as the parameters in the Dirichlet prior, from which base probabilities can be sampled. 

For example, if the $j$th column of $\nps'$ is (1200.45, 200.09, 100.74, 121.11, 0), then possible base probabilities could be (0.743, 0.126, 0.062, 0.070, 0), (0.739, 0.121, 0.066, 0.074, 0), or (0.744, 0.128, 0.056, 0.071, 0).
In contrast, if the $j$th column of $\nps'$ were (12.4, 1.2, 0.4, 1.7, 0), then some samples from the Dirichlet distribution might be (0.759, 0.059, 0.029, 0.153, 0), (0.916, 0.065, 0.009, 0.009, 0), or (0.779, 0.117, 0, 0.104, 0).
The lower coverage is incorporated into the analysis by the increase in the variance of the Dirichlet distribution.

In a maximum likelihood framework, this procdedure is similar to bootstrapping.
In fact, the ultimate effect of this is to decrease the bootstrap confidence to a level that is more in line with the actual uncertainty in the data.

In a Bayesian framework, the Dirichlet-multinomial sampling could be incorporated as prior distribution on each nucleotide.
For large collections of large sequences in an Markov Chain Monte Carlo algorithm, this increases the dimensionality dramatically.


In the next section, we provide a proposal for reducing the number of sequences that must be sampled.


## Reducing computational burden with sequence-level uncertainty

Suppose we wish to use a single sequence to estimate some parameter $\theta$ (for instance, the p-value or Bayes Factor for whether a sequence is a particular variant or the lineage assignment for grafting onto a pre-computed tree) and we are interested in the effect that uncertainty has on this estimate.
Computational burden can be partially alleviated by incorporating sequence-level uncertainty.

To start with, the most likely sequences must be calculated.
This could be done by calculating all of the sequences, comparing their likelihoods (labelled $a_i$), and putting them in order ($a_{(i)}$), but this is not necessary.
Instead, the most likely sequence is already known (the consensus sequence).
The second most likely sequence can be found by substituting the base call that was least likely (but still called) with the second most likely base call.


The calculation of the likelihood will be identical in these two cases except for the one substitution.
Instead of calculating the entire likelihood (which can be numerically unstable since there are nearly 30,000 base pairs), the difference in the likelihood can be calculated.

Consider the two vectors of length $\ell$, $M$ and $m$, where the $j$th element of $M$, $M_j$, represents the largest value in the $j$th column of $\nps$ and $m_j$ represents the second largest value.
The likelihood of the consensus sequence is $a_{(0)} = \prod_{j=1}^NM_j$.
Let $k$ be the index of the smallest value in $M$, then the second most likely sequence has likelihood $a_{(1)} = \prod_{j\ne k}M_jm_k$.
The change in likelihood is $a_{(0)}/a_{(1)} = M_k/m_k$, which implies that $a_{(1)} = a_{(0)}m_k/M_k$.

This process does not necessarily continue in a straightforward manner.
It is unclear whether retaining the first substitution and making a second substitution at a new site will lead to a larger change in the likelihood than reverting the first substitution and making a substitution at two new sites.
The differences in likelihood derived above provide a very fast comparison of likelihoods, though, so it is not expensive to calculate both possible situations.
Because of this speed-up, it is computationally feasible to simply calculate the likelihood for all substitutions involving the $d$ least likely base calls that made it into the conseq and then sort them out later.
There are $2^d$ possible combinations of substitutions for $d$ bases, which means that $d=12$ will provide the 4,096 most likely sequences.

For an arbitrary set of substitutions $\mathcal{I}$ relative to the conseq, the likelihood can be calculated as $a_{(0)}\prod_{j\in \mathcal{I}}m_j/M_k$.
To greatly simplify the calculation of the likelihood, only the $d$ locations need to be considered.

Suppose we are using these sequences to estimate some parameter $\theta$.
The sequences can be ordered according to their uncertainty, with the most likely sequence having uncertainty $a_{(1)}$, the second-most likely sequence $a_{(2)}$, etc.
The analysis can be performed using the most likely sequence, resulting in an estimate $\theta_{(0)}$.
The analysis can be re-run with the second-most likely sequence, $\theta_{(1)}$.
Given just these two estimates, a weighted estimate of $\theta$ given the 2 most likely sequences could be found as:


\begin{align}
\label{eq:orderedsequences}
\theta_{(0)} &= \theta,\\
\hat\theta_{(1)} &\approx \theta_{(0)} a_{(0)} + \theta_{(1)}a_{(1)}\
\end{align}

In \ref{eq:orderedsequences}, $\theta$ is the estimate from the consenses sequence ($a_{(0)}$). 
The third-most likely sequence can be analysed, resulting in $\hat\theta_{(2)}$.
This process can be repeated until new estimates do not have any impact on the results (conversion criteria could be set, but a graphical representation of $\theta_{(i)}$ would be more informative).
If convergence is not clearly met, the $d$ can be increased until enough sequences are obtained.

With this algorithm, the computational burden is separated into calculating the sequence liklihood for the most likely sequences and running the analysis enough times to be confident convergence is acheived.

For analyses that require a set of sequences, the likelihoods for the set of sequences can be calculated as the product of the likelihoods (or, for computational stability, the sum of the log-likelihoods) of the individual sequences.
The algorithm above can be adjusted accordingly.



# Application: SARS-CoV-2 lineage assignemnt

In this section, we demonstrate the re-sampling method on lineage assignment of SARS-CoV-2.
Sequences are sampled from $\nps'$ and then assigned a lineage based on the current state-of-the-art in phylogenetic analyis.

We use the lineage designations described in @rambautDynamicNomenclatureProposal2020 and assign our sequences to lineages using the pangoLEARN tool (pangolin version 2.3.2, pangoLEARN version 2021-02-21) that the authors have made available ([github.com/cov-lineages/pangolin](github.com/cov-lineages/pangolin)).
This tool uses a decision tree model to determine which lineage a given sequence is most likely to belong to.
The tool also reports a bootstrap support probability as a measurement of the tool's confidence in its assignment.
We demonstrate that even the best available tools are underestimating the variance and therefore producing overconfident conclusions.

## Data

The data for this application were downloaded from NCBI's SRA web interface ([https://www.ncbi.nlm.nih.gov/sra/?term=txid2697049](https://www.ncbi.nlm.nih.gov/sra/?term=txid2697049)).
Search results were filtered to only include runs that had SAM files.
To select which runs to download, a selection of 5-10 files from each of 20 non-sequential results pages were chosen.
Once collecting the run accession numbers from the search results, an R script was run to download the relevant files and check that all information was complete.
23 out of 275 files were labelled incomplete due to having too few reads (possibly because the download timed out) or not containing a CIGAR string.
The GISAID accession numbers for the seqeunces we used are provided in the appendix.

There was no particular reason for choosing any given file, but the resulting data should not be viewed as a random sample.
Each result page likely includes several runs from the same study, and runs were chosen arbitrarily within each result page.
We were not attempting a completely random sampling strategy, we simply wanted a collection of runs on which to demonstrate our methods.

## Re-sampling the call probability matrix 


Since pangoLEARN is a pre-trained model, assigning lineage designations to a large number of resampled genome sequences is not computationally burdensome.
Sampling 5,000 different sequences from a call probability matrix is reasonable, even on a mid-range consumer laptop.

For this analysis we use the most basic resampling strategy described above.
We simply draw probabilities from the Dirichlet distribution with parameters defined by $\nps'$, sample base calls from the multinomial distribution, then use pangoLEARN to determine the lineage assignment.


In \ref{fig:covidcalls}, each row represents 5,000 resamples from one SAM file.
Each bar represents one lineage assignment from pangolin.
The width of the bars represents the number times a re-sampled sequence resulted in the labelled lineage designation.
There were 95 sequences total, but we only plotted ones where the second most common lineage designation had more than 250 observations.
Results for all lineages can be found in Table \ref{tab:pango} in the Appendix. 


```{r sampled_bars, fig.height=11, fig.width=13, fig.cap='\\label{fig:covidcalls}Distribution of called lineages from pangolin. Red bars indicate the lineage of the most probable sequence. Any lineage with fewer than 100 observations was grouped into the "Other" category.'}
max_label <- 250
other_label <- 100

par(mfrow = c(21, 1), mar = c(0.05, 7.75, 0.05, 0.05))
if (exists("seq_info")) rm(seq_info)
for (i in seq_along(taxons)) {
    pang <- lins[lins$taxon == taxons[i], ]
    called <- pang$lineage[pang$sample == 0][1]
    pangtab <- sort(table(pang$lineage), decreasing = TRUE)

    # Prep the data for a nicely formatted table
    # Subtract one because of the conseq.
    seq_info_i <- data.frame(
        called = called,
        mode = names(pangtab)[1],
            mode_n = pangtab[1] - 1,
            perc = round(100 * (pangtab[1] - 1) / (sum(pangtab) - 1), 2),
        runner_up = names(pangtab)[2],
            ru_n = pangtab[2],
        unique = length(pangtab), atoms = sum(pangtab == 1))
    seq_info_i$taxon <- taxons[i]

    if (!exists("seq_info")) {
        seq_info <- seq_info_i
    } else {
        seq_info <- bind_rows(seq_info, seq_info_i)
    }

    colvec <- rep("grey", length(pangtab))
    colvec[which(names(pangtab) == called)] <- "red"

    n <- sum(pangtab > max_label)
    if (n > 1) {
        add_other <- FALSE
        if (sum(pangtab < other_label) > 10) {
            add_other <- TRUE
            other_count <- sum(pangtab <= other_label)
            pangtab <- c(pangtab[pangtab > other_label],
                c("other" = sum(pangtab[pangtab <= other_label])))
            colvec[which(names(pangtab) == "other")] <- "black"
        }
        barlabx <- c(0, cumsum(pangtab[1:(n - 1)])) +
            pangtab[1:n] / 2
        barlabels <- names(pangtab)[1:n]
        barlens <- sapply(gregexpr("\\.", barlabels), length)
        for (j in seq_along(barlabels)) {
            if (pangtab[j] < 400 & barlens[j] >= 2) {
                barsplit <- strsplit(barlabels[j], split = "\\.")[[1]]
                barn <- length(barsplit)
                half <- floor(barn / 2)
                barlabels[j] <- paste0(
                    paste(barsplit[1:half], collapse = "."),
                    ".\n",
                    paste(barsplit[(half + 1):barn], collapse = ".")
                )
            }
        }

        barplot(as.matrix(pangtab),
            col = colvec, hori = TRUE, axes = FALSE)
        text(barlabx, 0.7, barlabels, cex = 1.5)
        if (add_other) {
            text(x = sum(pangtab) - pangtab["other"] / 2,
            y = 0.7, col = "white", cex = 1.5,
            label = paste0("Others:\n", other_count))
        }
        mtext(side = 2, cex = 1, las = 1,
            text = paste(substr(taxons[i], 1, 3),
                substr(taxons[i], 4, 20), sep = "\n"))
        abline(v = seq(0, 10000, 1000), lty = 2)
        "pretty_labels <- seq(0, sum(pangtab),
                by = ifelse(sum(pangtab) < 2000, 100, 1000))
        mtext(side = 1,
            at = pretty_labels,
            text = pretty_labels,
            line = 0,
            cex = 0.75
        )"
    }
}
```

Figure \ref{fig:covidcalls} shows that the consensus sequence almost always is assigned the same lineage as the majority of the resamples; the full results are in Table \ref{tab:pango}.
The proportion of resamples with the same lineage as the consensus sequence is very rarely 100\% and can be as low as 22.09\%.
It is noteworthy that the only times where 100\% of resampled sequences agreed are when the lineage call was "None" or for the lineage labelled B.1.1.7, which is a significantly more infectious lineage and is of special concern to health authorities.

Initial analyses used pangolin version 2.0.8, which used multinomial logistic regression, rather than decision trees, to assign lineages.
This procedure resulted in bootstrap support values that varied from 0.6 to 1.
After upgrading to pangolin version 2.3.2, which uses a decision tree model, all bootstrap probabilities are reported as 1.


# Application: Clock rate estimation for SARS-CoV-2

TODO: Better descriptions (more precise, more complete). 
TODO: State problem of different coverage *over time*, use that to motivate data filtering.

The clock rate (slope of a root-to-tip regression) for SARS-CoV-2 is commonly estimated as a fixed rate near 0.001 mutations per site per year (specific estimates given below).
Using the same resampling methods as above, we estimate a clock rate for trees estimated from each of 50 resamples and for the tree estimated based on the reported conseqs.

To obtain the data, we sampled genomes uniformly from each month of recorded data in GenBank, using filters to ensure that the genomes were complete and had an associated Short Run Accession file (\ie they had an available SAM file).
We further had to filter out SAM files that were incomplete or did not contain the CIGAR strings necessary for alignment.
The associated GISAID accession numbers are provided in the Appendix. TODO: add accession numbers.

Our re-sampling method will, by definition, introduce other possible mutations beyond what the conseq suggests.
Because of this, the apparent number of mutations between a re-sampled genome and the estimated root is a function of the coverage, with more positions read or more uncertainty in the sequence leading to artificially inflated phylogenetic branch lengths.
This implies that the estimates of the time for the most recent common ancestor are not reliable.
However, assuming that the sequences have comparable levels of uncertainty, each branch increases proportionally and the clock rate should not be affected. 

The sequences that we acquired did not have comparable levels of uncertainty; the viruses sampled early in the pandemic had considerable higher uncertainty.
To account for this, we found the sum of $\nps'$ for each sequence and applied Statistical Process Control techniques to ensure that all of the sequences had a similar level of coverage.
In particular, we found the mean of the coverage of the sequences in our data set, $\bar c$, and the standard deviation of the coverage, $s$. 
We removed any sequences outside of $\bar x \pm 3 s$, recalculated $\bar x$ and $s$, and iterated the removal process until all sequence coverages were within the bounds.

We acknowledge that it is not ideal to remove data, but original data we were working with had a systematic difference in coverage/quality over time.
This systematic trend leads to a bias in the estimate of the clock rate since the number of mutations in resampled sequences becomes a function of time.
TODO: Not all re-samples converged. Is this a problem?

The clock rate was estimated using TreeTime (@sagulenkoTreeTimeMaximumlikelihoodPhylodynamic2018). 
We recorded the standard error from the time tree constructed using the consensus sequences, then compared this to the standard deviations of the estimated clock rates in the resampled sequences.
Rate estimates from @ducheneTemporalSignalPhylodynamic2020 (n=122), @choudharySevereAcuteRespiratory2021 (n=261), @songGenomicEpidemiologySARSCoV22021 (n=29), @niePhylogeneticPhylodynamicAnalyses2020 (n=112), and @geidelbergGenomicEpidemiologyDensely2021 (n=77) are also labelled on the plot with purple error bars for 95% Bayesian Credible Intervals (BCI) or Highest Posterior Density (HPD), indicating that the rates and errors from each root-to-tip regression are in line with other published results.
Figure \ref{fig:RTT_slope} demonstrates that the estimated evolutionary rates have an average close to the rate estimated from the conseqs, but all of the error bars miss the variation due to sequence uncertainty.

```{r RTT_slope, warning=FALSE, results='hide', fig.cap="\\label{fig:RTT_slope}Clock rates (slope) and 95% Confidence Intervals for the collections of re-sampled sequences. The red line and red shaded region are the clock rate and 95% CI for the consensus sequences. The purple points and error bars are the clock rates and error intervals (either Bayesian Credible Interval or Highest Posterior Probability) from published studies, as labelled. The re-sampled sequences are in line with the consensus sequences as well as the published sequences, but represent a much larger variation due to the uncertainty in the original genome sequences."}
library(dplyr)
library(lubridate)
library(here)
library(ggplot2)
library(ggrepel)

# raw (conseqs), tree_dirs (resampled), and 
    # lit_clock (published)
load(here("RTT", "RTT_output.RData"))

ggplot() +
    theme_bw() +
    geom_hline(yintercept = 0, colour = "darkgrey") +
    geom_hline(yintercept = raw$slope, col = "red") +
    geom_rect(
        mapping = aes(xmin = -10, xmax = 100,
            ymin = raw$slope - 1.96*raw$sd, ymax = raw$slope + 1.96*raw$sd),
        fill = "red", alpha = 0.2) +
    geom_errorbar(
        mapping = aes(x = as.numeric(rank),
            ymin = slope - 1.96*sd,
            ymax = slope + 1.96*sd),
        data = tree_dirs) +
    geom_point(
        mapping = aes(x = as.numeric(rank), y = slope),
        data = tree_dirs) +
    geom_errorbar(
        mapping = aes(x = lit_clock_x,
            ymin = as.numeric(lo95),
            ymax = as.numeric(hi95)),
        data = lit_clock,
        colour = "darkorchid") +
    geom_point(
        data = lit_clock,
        mapping = aes(x = lit_clock_x, 
            y = as.numeric(clock)),
        colour = "darkorchid"
        ) +
    geom_text(data = lit_clock,
        mapping = aes(
            y = as.numeric(hi95), 
            x = lit_clock_x,
            label = study),
        colour = "black", 
        angle = 90,
        hjust = -0.05,
        size = 2.5
        ) +
    scale_colour_viridis_d() +
    coord_cartesian(xlim = c(0, 50), ylim = c(-0.001, 0.0035)) +
    labs(x = "Index (Ordered by Slope)", 
        y = "Slope +/- 1.96 SD",
        title = "Root-to-Tip Slopes from collections of re-sampled genomes",
        subtitle = "Compared to conseqs (red) and published estimates (purple)") +
    theme(legend.position = "none")

```



# Conclusions

The short run files produced by next generation sequencing platforms include valuable information about the quality of base calls which can easily be propagated into analyses.
The primary contribution of this research is the construction of the probability sequence, which allows for a wide variety of future research directions.
One such direction is re-sampling, which allows a more honest appraisal of the variance in the estimates (or provides a reasonable prior distribution in a Bayesian setting), while comparing results for the most likely sequences provide a measure of robustness to sequence uncertainty.

Our proposed methods can result in a drastic increase in computational expense.
Even the method base on ordering the sequences by likelihood can result in re-running the analysis numerous times.
However, we have demonstrated that the uncertainty in the sequences themseleves can lead to major changes to the interpretations of the results.
The so-called "consensus sequence" is simply the most likely sequence, and the reported uncertainty is not merely an academic curiousity.

Our analysis focused on phylogenetic trees based on HIV sequences as well as designation of lineages according to the Pangolin model.
The importance of incorporating sequence uncertainty is not confined to these applications; any analysis involving sequenced genomes would benefit from some method of incoporating the uncertainty or including some measure of robustness.
For example, the estimated frequency of alleles in the population could be used as the probability sequence, then propagated into further analysis.

Our method does not preclude tertiary analyses to test for systematic errors or deviations from a Mendelian inheritance pattern assumption.
We cannot account for systematic errors, such as those present due to human errors (\eg mis-calibrated machines or that one lab in @blank that kept giving the same mutations). TODO: Ask art about the bad sequences from the bad lab
Our method allows for adjustments of the base call quality score, such as in @brockmanQualityScoresSNP2008, as well as more sophisticated definitions of genome likelihoods. TODO: some references here.


We have developed an algorithm to include insertion events in a re-sampling scheme, but many of the resultant sequences were not mappable to known sequences.
The pangolin lineage assignment system appears to treat insertions differently from single nucleotide polymorphisms, and our method of sampling insertions is incompatible with their treatment of insertions in lineage assignment operations.
This is potentially because the sampled base pair at any given location is independent of each other location, and the insertions observed in real-world data are possibly always associated with particular mutations elsewhere.

This study should not be taken in any way as a criticism of the pangolin lineage assignment procedure.
Rather, pangolin was chosen as it is a state-of-the art tool based on the best available algorithms for phylogeny reconstruction.
The phylogeny created by this team has been a vital resource for researchers and for public health professionals.
In particular, their label for the current Variants of Concern (VOCs), especially B.1.1.7, are the labels being used worldwide by news organizations.

TODO: Add more conclusions:
- Resample to get variance around representative sequence for a given lineage (\eg HIV).
    - Can be done with just FASTQ files (less accurate).
- Given a canonical reference seqeunce for a lineage, find level of entropy that makes it a different sequence.
- Conclusions from clock rate estimation.
- Phylogenies have been estimated based on uncertain sequence information in @rossOncoNEMInferringTumor2016, @jahnTreeInferenceSinglecell2016, and @zafarSiFitInferringTumor2017 but the uncertainty is not derived from base quality scores. An extension of these methods to incorporate the base quality scores may be a worthwhile research direction.
- Document problems with root-to-tip regression.

# Appendix: Results for all lineages

\LTcapwidth=\textwidth

TODO: Move this to a separate file

\scriptsize

```{r big_kable, fig.cap='\\label{tab:pango}Results for re-sampling analysis all accession numbers in our analysis. The "Conseq" column refers to the lineage designated to the consensus sequence. "Mode" refers to the most common lineage across all re-samples, with "Mode #" indicating the number of re-samples with this lineage. "Runner Up" and "RU #" are the second most likely lineage designation and the number with that designation, respectively. "Unique" is the total number of unique lineage designations, and "Atoms" are the total number of lineage designations that were only observed for a single re-sample.'}
seq_info$taxon <- taxons
seq_info <- arrange(seq_info, mode, mode_n) %>%
    select(taxon, everything())
seq_info <- rename(seq_info, Accession = taxon, Conseq = called,
    Mode = mode, `Mode #` = mode_n, `Mode #/N` = perc,
    `Runner-Up` = runner_up, `RU #` = ru_n,
    Unique = unique, Atoms = atoms)
knitr::kable(seq_info, row.names = FALSE)
```

\normalsize

# Bibliography


